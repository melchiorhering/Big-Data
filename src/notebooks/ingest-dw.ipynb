{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data Warehouse\n",
    "\n",
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from typing import List, Optional, Any\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType\n",
    "from pyspark.sql import SparkSession, DataFrame as SparkDataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from duckdb import DuckDBPyConnection, DuckDBPyRelation\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import duckdb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyspark.__version__)\n",
    "print(duckdb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_database = \"../orchestration/db/bigdata.duckdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuckDBContext:\n",
    "    def __init__(self, db_path: str):\n",
    "        self.db_path = db_path\n",
    "\n",
    "    def __enter__(self) -> \"DuckDBContext\":\n",
    "        self.conn = duckdb.connect(database=self.db_path, read_only=False)\n",
    "        return self\n",
    "\n",
    "    def __exit__(\n",
    "        self,\n",
    "        exc_type: Optional[type],\n",
    "        exc_val: Optional[Exception],\n",
    "        exc_tb: Optional[object],\n",
    "    ) -> None:\n",
    "        self.conn.close()\n",
    "\n",
    "    def save_to_duckdb(self, df, table_name: str) -> None:\n",
    "        # If the DataFrame is a Polars DataFrame, convert it to an Arrow table\n",
    "        if isinstance(df, pl.DataFrame):\n",
    "            df = pa.Table.from_pandas(df.to_pandas())\n",
    "        # If the DataFrame is a Spark DataFrame, convert it to an Arrow table\n",
    "        elif isinstance(df, pyspark.sql.DataFrame):\n",
    "            df = pa.Table.from_batches(df._collect_as_arrow())\n",
    "        # Convert the Arrow table to a DuckDB DataFrame\n",
    "        df = self.conn.from_arrow(df)\n",
    "\n",
    "        df.create(table_name)\n",
    "        row_count = self.conn.query(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "        print(f\"CREATED TABLE: {table_name} WITH {row_count} ROWS!\")\n",
    "\n",
    "    def show_n(self, table_name: str, n: int = 10):\n",
    "        try:\n",
    "            result = self.conn.execute(f\"SELECT * FROM {table_name} LIMIT {n}\")\n",
    "\n",
    "            print(result.pl())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Usage\n",
    "# with DuckDBContext(\"../orchestration/db/bigdata.duckdb\") as con:\n",
    "#     # Use 'con' for operations\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_name(file_name: str) -> str:\n",
    "    return file_name.rsplit(\".\", 2)[0]\n",
    "\n",
    "\n",
    "def create_url(endpoint: str) -> str:\n",
    "    \"\"\"\n",
    "    Create Url\n",
    "\n",
    "    :param str endpoint: download endpoint\n",
    "    :return str: full url\n",
    "    \"\"\"\n",
    "    return f\"https://datasets.imdbws.com/{endpoint}\"\n",
    "\n",
    "\n",
    "def download_file(url, filename):\n",
    "    print(f\"Downloading file: {filename}\")\n",
    "\n",
    "    response = urllib.request.urlopen(url)\n",
    "\n",
    "    # Get the total file size\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    progress = tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=filename)\n",
    "\n",
    "    chunk_size = 1024  # you can change this to larger if you want\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        while True:\n",
    "            chunk = response.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "            progress.update(len(chunk))\n",
    "    progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Cluster Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark Cluster\n",
    "# spark = (\n",
    "#     SparkSession.builder.master(\"spark://spark:7077\")\n",
    "#     .appName(\"Spark-ETL\")\n",
    "#     .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# Connect to local Spark Sessions\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\").appName(\"Spark-ETL\")\n",
    "    # .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to Data Warehouse\n",
    "\n",
    "## Initial Data\n",
    "\n",
    "#### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files that match the pattern\n",
    "csv_files = glob.glob(\"../../data/train-*.csv\")\n",
    "print(csv_files)\n",
    "\n",
    "# Load all CSV files in the data directory into a dataframe\n",
    "# Specify '\\\\N' as a null value\n",
    "# Ignore the header and infer the schema from data\n",
    "train_spark_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../../data/train-*.csv\", nullValue=\"\\\\N\")\n",
    ")\n",
    "# Drop the first column\n",
    "train_spark_df = train_spark_df.drop(\"_c0\")\n",
    "train_spark_df.describe().show()\n",
    "# Print the dataframe\n",
    "train_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Polars to retrieve the directing data\n",
    "# Load and parse the JSON file\n",
    "with open(\"../../data/directing.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "movies_polars_df = pl.from_dict(data[\"movie\"]).transpose().rename({\"column_0\": \"movie\"})\n",
    "directors_polars_df = (\n",
    "    pl.from_dict(data[\"director\"]).transpose().rename({\"column_0\": \"director\"})\n",
    ")\n",
    "directing_polars_df = pl.concat(\n",
    "    [\n",
    "        movies_polars_df,\n",
    "        directors_polars_df,\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")\n",
    "directing_polars_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/writing.json\") as f:\n",
    "    data = json.load(f)\n",
    "writing_json = spark.sparkContext.parallelize(data)\n",
    "writing_spark_df = spark.read.json(writing_json)\n",
    "writing_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into DuckDB Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDBContext to add pyspark tables to DuckDB\n",
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    ctx.save_to_duckdb(train_spark_df, \"imdb_train\")\n",
    "    ctx.show_n(\"imdb_train\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(directing_polars_df, \"imdb_directors\")\n",
    "    ctx.show_n(\"imdb_directors\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(writing_spark_df, \"imdb_writing\")\n",
    "    ctx.show_n(\"imdb_writing\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading\n",
    "\n",
    "Run this cell once, otherwise you'll keep downloading the same files over and over...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_imdb = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    \"title.episode.tsv.gz\",\n",
    "    \"title.principals.tsv.gz\",\n",
    "    \"title.ratings.tsv.gz\",\n",
    "]\n",
    "\n",
    "# RUN THIS ONCE!\n",
    "# # Download the files\n",
    "# for ds in extra_imdb:\n",
    "#     # Create an instance of the IMDB class with the desired endpoint\n",
    "#     download_url = create_url(ds)\n",
    "\n",
    "#     filepath = f\"../../data/extra/{ds}\"  # Local fp\n",
    "#     # Use the function to download the file\n",
    "#     download_file(download_url, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading with Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DuckDBContext(\"../orchestration/db/bigdata.duckdb\") as ctx:\n",
    "    train_ids = ctx.conn.execute(\"SELECT tconst FROM imdb_train\").fetchdf()\n",
    "    # Convert the pandas DataFrame to a PySpark DataFrame\n",
    "    train_ids_spark = spark.createDataFrame(train_ids)\n",
    "    train_ids_spark.show(5)\n",
    "\n",
    "    for ds in extra_imdb:\n",
    "        print(f\"Reading file: {ds}\")\n",
    "        table_name = f\"extra.{get_table_name(ds)}\"\n",
    "        print(\"Creating table: \", table_name)\n",
    "\n",
    "        if ds == \"name.basics.tsv.gz\":\n",
    "            # Load all TSV.GZ files in the data directory into a dataframe\n",
    "            spark_df = spark.read.csv(\n",
    "                f\"../../data/extra/{ds}\", header=True, sep=\"\\t\", nullValue=\"\\\\N\"\n",
    "            )\n",
    "            spark_df.show(5)\n",
    "        elif ds == \"title.akas.tsv.gz\":\n",
    "            # Load all TSV.GZ files in the data directory into a dataframe\n",
    "            spark_df = spark.read.csv(\n",
    "                f\"../../data/extra/{ds}\", header=True, sep=\"\\t\", nullValue=\"\\\\N\"\n",
    "            )\n",
    "            spark_df.show(5)\n",
    "            print(\"Joining with train_ids_spark on tconst <==> titleId\")\n",
    "            joined_spark_df = (\n",
    "                train_ids_spark.join(\n",
    "                    spark_df, train_ids_spark.tconst == spark_df.titleId, how=\"left\"\n",
    "                )\n",
    "                .drop(\"titleId\")\n",
    "                .show(5)\n",
    "            )\n",
    "            print(type(joined_spark_df))\n",
    "\n",
    "            ctx.save_to_duckdb(joined_spark_df, table_name)\n",
    "            ctx.show_n(table_name, 5)\n",
    "\n",
    "        else:\n",
    "            # Load all TSV.GZ files in the data directory into a dataframe\n",
    "\n",
    "            spark_df = spark.read.csv(\n",
    "                f\"../../data/extra/{ds}\", header=True, sep=\"\\t\", nullValue=\"\\\\N\"\n",
    "            )\n",
    "            spark_df.show(5)\n",
    "            print(\"Joining with train_ids_spark on tconst\")\n",
    "            joined_spark_df = train_ids_spark.join(\n",
    "                spark_df, on=\"tconst\", how=\"left\"\n",
    "            ).show(5)\n",
    "            print(type(joined_spark_df))\n",
    "\n",
    "            ctx.save_to_duckdb(joined_spark_df, table_name)\n",
    "            ctx.show_n(table_name, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
