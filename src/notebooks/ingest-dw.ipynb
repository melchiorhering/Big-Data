{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data Warehouse\n",
    "\n",
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import Optional\n",
    "from lib.duckdbcontext import DuckDBContext\n",
    "import polars as pl\n",
    "import pyspark\n",
    "import opendatasets as od\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    explode,\n",
    "    split,\n",
    "    udf,\n",
    "    size,\n",
    "    regexp_replace,\n",
    "    when,\n",
    "    array,\n",
    "    countDistinct,\n",
    "    col,\n",
    ")\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import ast\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import duckdb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyspark.__version__)\n",
    "print(duckdb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_database = \"../orchestration/db/bigdata.duckdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Cluster Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Existing Spark Cluster\n",
    "# spark = (\n",
    "#     SparkSession.builder.master(\"spark://spark:7077\")\n",
    "#     .appName(\"Spark-ETL\")\n",
    "#     .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# Connect to local Spark Sessions\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\")\n",
    "    .appName(\"Spark-ETL\")\n",
    "    # .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to Data Warehouse\n",
    "\n",
    "## Initial Data\n",
    "\n",
    "#### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files that match the pattern\n",
    "csv_files = glob.glob(\"../../data/train-*.csv\")\n",
    "print(csv_files)\n",
    "\n",
    "# Load all CSV files in the data directory into a dataframe\n",
    "# Specify '\\\\N' as a null value\n",
    "# Ignore the header and infer the schema from data\n",
    "train_spark_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../../data/train-*.csv\", nullValue=\"\\\\N\")\n",
    ")\n",
    "# Drop the first column\n",
    "train_spark_df = train_spark_df.drop(\"_c0\")\n",
    "train_spark_df.describe().show()\n",
    "# Print the dataframe\n",
    "train_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Polars to retrieve the directing data\n",
    "# Load and parse the JSON file\n",
    "with open(\"../../data/directing.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "movies_polars_df = pl.from_dict(data[\"movie\"]).transpose().rename({\"column_0\": \"movie\"})\n",
    "directors_polars_df = (\n",
    "    pl.from_dict(data[\"director\"]).transpose().rename({\"column_0\": \"director\"})\n",
    ")\n",
    "directing_polars_df = pl.concat(\n",
    "    [\n",
    "        movies_polars_df,\n",
    "        directors_polars_df,\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")\n",
    "directing_polars_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/writing.json\") as f:\n",
    "    data = json.load(f)\n",
    "writing_json = spark.sparkContext.parallelize(data)\n",
    "writing_spark_df = spark.read.json(writing_json)\n",
    "writing_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into DuckDB Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDBContext to add pyspark tables to DuckDB\n",
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    ctx.save_to_duckdb(train_spark_df, \"imdb_train\")\n",
    "    ctx.show_n(\"imdb_train\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(directing_polars_df, \"imdb_directors\")\n",
    "    ctx.show_n(\"imdb_directors\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(writing_spark_df, \"imdb_writing\")\n",
    "    ctx.show_n(\"imdb_writing\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_name(file_name: str) -> str:\n",
    "    return file_name.rsplit(\".\", 2)[0]\n",
    "\n",
    "\n",
    "def create_url(endpoint: str) -> str:\n",
    "    \"\"\"\n",
    "    Create Url\n",
    "\n",
    "    :param str endpoint: download endpoint\n",
    "    :return str: full url\n",
    "    \"\"\"\n",
    "    return f\"https://datasets.imdbws.com/{endpoint}\"\n",
    "\n",
    "\n",
    "def download_file(url, filename):\n",
    "    print(f\"Downloading file: {filename}\")\n",
    "\n",
    "    response = urllib.request.urlopen(url)\n",
    "\n",
    "    # Get the total file size\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    progress = tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=filename)\n",
    "\n",
    "    chunk_size = 1024  # you can change this to larger if you want\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        while True:\n",
    "            chunk = response.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "            progress.update(len(chunk))\n",
    "    progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dowloading\n",
    "\n",
    "Run this cell once, otherwise you'll keep downloading the same files over and over...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_imdb = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    # \"title.episode.tsv.gz\", # we have only movie data\n",
    "    \"title.principals.tsv.gz\",\n",
    "    \"title.ratings.tsv.gz\",\n",
    "]\n",
    "\n",
    "# RUN THIS ONCE!\n",
    "# Download the files\n",
    "for ds in extra_imdb:\n",
    "    # Create an instance of the IMDB class with the desired endpoint\n",
    "    download_url = create_url(ds)\n",
    "\n",
    "    filepath = f\"../../data/extra/{ds}\"  # Local fp\n",
    "    # Use the function to download the file\n",
    "    download_file(download_url, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading with Spark\n",
    "\n",
    "RUN THIS ONCE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    train_ids = ctx.conn.execute(\"SELECT tconst FROM imdb_train\").fetchdf()\n",
    "    train_ids_spark = spark.createDataFrame(train_ids)\n",
    "\n",
    "    for ds in extra_imdb:\n",
    "        table_name = f\"extra.{get_table_name(ds)}\".replace(\".\", \"_\")\n",
    "\n",
    "        # Load a small subset of the data to infer the schema\n",
    "        subset = spark.read.csv(\n",
    "            f\"../../data/extra/{ds}\",\n",
    "            header=True,\n",
    "            sep=\"\\t\",\n",
    "            nullValue=\"\\\\N\",\n",
    "            inferSchema=True,\n",
    "        ).limit(1000)\n",
    "\n",
    "        # Extract the schema from the subset\n",
    "        schema = subset.schema\n",
    "\n",
    "        # Load all TSV.GZ files in the data directory into a dataframe with the inferred schema\n",
    "        spark_df = spark.read.csv(\n",
    "            f\"../../data/extra/{ds}\",\n",
    "            header=True,\n",
    "            sep=\"\\t\",\n",
    "            nullValue=\"\\\\N\",\n",
    "            schema=schema,\n",
    "        )\n",
    "        spark_df.show(5)\n",
    "\n",
    "        spark_df_columns = spark_df.columns\n",
    "\n",
    "        if \"titleId\" in spark_df_columns:\n",
    "            filtered_spark_df = spark_df.join(\n",
    "                train_ids_spark, train_ids_spark.tconst == spark_df.titleId, \"inner\"\n",
    "            )\n",
    "            filtered_spark_df = filtered_spark_df.drop(\"titleId\")\n",
    "        elif \"knownForTitles\" in spark_df_columns:\n",
    "            # Split the knownForTitles column into multiple rows\n",
    "            spark_df = spark_df.withColumn(\n",
    "                \"knownForTitles\", explode(split(spark_df[\"knownForTitles\"], \",\"))\n",
    "            )\n",
    "\n",
    "            # Select the values that are in both train_ids_spark and spark_df\n",
    "            filtered_spark_df = spark_df.join(\n",
    "                train_ids_spark,\n",
    "                spark_df.knownForTitles == train_ids_spark.tconst,\n",
    "                \"inner\",\n",
    "            )\n",
    "        elif \"tconst\" in spark_df_columns:\n",
    "            filtered_spark_df = spark_df.join(train_ids_spark, \"tconst\", \"inner\")\n",
    "\n",
    "        ctx.save_to_duckdb(filtered_spark_df, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert strings to lists\n",
    "def parse_list(s):\n",
    "    return s.strip(\"[]\").split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letterboxd Movie Ratings Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/samlearner/letterboxd-movie-ratings-data/download?datasetVersionNumber=6\",\n",
    "    data_dir=\"../../data/extra\",\n",
    ")\n",
    "# Oscar Award Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/unanimad/the-oscar-award\",\n",
    "    data_dir=\"../../data/extra\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all CSV files\n",
    "for file_path in csv_files:\n",
    "    # Check if 'users_export.csv' or 'ratings_export.csv' is part of the file name\n",
    "    if \"users_export.csv\" in file_path or \"ratings_export.csv\" in file_path:\n",
    "        # If the file exists, remove it\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"{file_path} removed successfully.\")\n",
    "        else:\n",
    "            print(f\"{file_path} does not exist.\")\n",
    "    else:\n",
    "        df = (\n",
    "            spark.read.csv(\n",
    "                file_path,\n",
    "                header=True,\n",
    "                inferSchema=True,\n",
    "            )\n",
    "            .limit(5)\n",
    "            .show(5)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert strings to lists\n",
    "def parse_list(s):\n",
    "    return s.strip(\"[]\").split(\", \")\n",
    "\n",
    "\n",
    "parse_list_udf = udf(parse_list, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def process_column(df, column_name):\n",
    "    # Remove the extra double quotes\n",
    "    df = df.withColumn(column_name, regexp_replace(df[column_name], '\"', \"\"))\n",
    "    print(f\"Size after removing double quotes: {df.count()} rows\")\n",
    "\n",
    "    # Filter the DataFrame to only include rows where the column is not null\n",
    "    df = df.filter(col(column_name).isNotNull())\n",
    "    print(f\"Size after filtering nulls: {df.count()} rows\")\n",
    "\n",
    "    # Convert the column to a list\n",
    "    df = df.withColumn(column_name, parse_list_udf(df[column_name]))\n",
    "    print(f\"Size after converting to list: {df.count()} rows\")\n",
    "\n",
    "    # Check if there are any arrays with multiple values\n",
    "    multi_value_rows = df.filter(size(df[column_name]) > 1)\n",
    "    print(\n",
    "        f\"Number of rows with multiple values in {column_name}: {multi_value_rows.count()}\"\n",
    "    )\n",
    "\n",
    "    # Explode the array into new rows\n",
    "    df = df.withColumn(column_name[:-1], explode(df[column_name]))\n",
    "    print(f\"Size after exploding list: {df.count()} rows\")\n",
    "\n",
    "    # Drop the original column\n",
    "    df = df.drop(column_name)\n",
    "    print(f\"Size after dropping original column: {df.count()} rows\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DuckDBContext\n",
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    train_ids = ctx.conn.execute(\"SELECT tconst FROM imdb_train\").fetchdf()\n",
    "    train_ids_spark = spark.createDataFrame(train_ids)\n",
    "\n",
    "    spark_df = spark.read.csv(\n",
    "        \"../../data/extra/the-oscar-award/the_oscar_award.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    # Save the DataFrame to DuckDB\n",
    "    ctx.save_to_duckdb(spark_df, \"the_oscar_award\")\n",
    "\n",
    "    # Read Movie Data & drop unnecessary columns\n",
    "    spark_df = spark.read.csv(\n",
    "        \"../../data/extra/letterboxd-movie-ratings-data/movie_data.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    ).drop(\"image_url\", \"imdb_link\", \"overview\", \"tmdb_id\", \"tmdb_link\")\n",
    "\n",
    "    # Filter on our Train IDs\n",
    "    filtered_spark_df = spark_df.join(\n",
    "        train_ids_spark, spark_df.imdb_id == train_ids_spark.tconst, \"inner\"\n",
    "    ).drop(\"imdb_id\")\n",
    "    filtered_spark_df.show(5)\n",
    "\n",
    "    # Process the genres, production_countries, and spoken_languages columns\n",
    "    for column in [\"genres\", \"production_countries\", \"spoken_languages\"]:\n",
    "        filtered_spark_df = process_column(filtered_spark_df, column)\n",
    "\n",
    "    filtered_spark_df.show(5)\n",
    "\n",
    "    # Save the DataFrame to DuckDB\n",
    "    ctx.save_to_duckdb(filtered_spark_df, \"letterboxd_movie_ratings_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
