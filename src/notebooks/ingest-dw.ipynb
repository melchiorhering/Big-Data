{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data Warehouse\n",
    "\n",
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import Optional\n",
    "from lib.duckdbcontext import DuckDBContext\n",
    "import polars as pl\n",
    "import pyspark\n",
    "import opendatasets as od\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    explode,\n",
    "    split,\n",
    "    udf,\n",
    "    size,\n",
    "    regexp_replace,\n",
    "    when,\n",
    "    array,\n",
    "    countDistinct,\n",
    "    col,\n",
    ")\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import ast\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import duckdb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "0.9.2\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)\n",
    "print(duckdb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_database = \"../orchestration/db/bigdata.duckdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Cluster Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 16:49:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Connect to Existing Spark Cluster\n",
    "# spark = (\n",
    "#     SparkSession.builder.master(\"spark://spark:7077\")\n",
    "#     .appName(\"Spark-ETL\")\n",
    "#     .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# Connect to local Spark Sessions\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\")\n",
    "    .appName(\"Spark-ETL\")\n",
    "    # .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to Data Warehouse\n",
    "\n",
    "## Initial Data\n",
    "\n",
    "#### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/train-8.csv', '../../data/train-2.csv', '../../data/train-7.csv', '../../data/train-5.csv', '../../data/train-3.csv', '../../data/train-4.csv', '../../data/train-1.csv', '../../data/train-6.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:49:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|   tconst|        primaryTitle|     originalTitle|         startYear|           endYear|    runtimeMinutes|          numVotes|\n",
      "+-------+---------+--------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|     7959|                7959|              3971|              7173|               786|              7946|              7169|\n",
      "|   mean|     NULL|   1231.388888888889|            1128.0|1997.9960964728843|1998.7633587786258|105.68713818273345| 29520.51081043381|\n",
      "| stddev|     NULL|   954.6947857755001|1038.0438333712118| 21.99534723241901|   21.895931761063| 25.39634772412447|114449.99384975343|\n",
      "|    min|tt0009369|\"Drágớn Báll Z: R...|     'A' gai wak 2|              1918|              1921|                45|            1001.0|\n",
      "|    max|tt9911196|             Ớútcást|        Üç Kagitçi|              2021|              2021|               551|         2503641.0|\n",
      "+-------+---------+--------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "|   tconst|        primaryTitle|  originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "|tt0014109|The Saga of Gösta...|           NULL|     1924|   NULL|           183|  1231.0| true|\n",
      "|tt0015064|      The Last Laugh|Der letzte Mann|     1924|   NULL|            77|    NULL| true|\n",
      "|tt0015841|        The Freshman|   The Freshman|     1925|   NULL|            77|  5374.0| true|\n",
      "|tt0017271|          By the Law|           NULL|     NULL|   1926|            80|  1057.0| true|\n",
      "|tt0018451|The Student Princ...|           NULL|     1927|   NULL|           106|  1459.0| true|\n",
      "+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all CSV files that match the pattern\n",
    "csv_files = glob.glob(\"../../data/train-*.csv\")\n",
    "print(csv_files)\n",
    "\n",
    "# Load all CSV files in the data directory into a dataframe\n",
    "# Specify '\\\\N' as a null value\n",
    "# Ignore the header and infer the schema from data\n",
    "train_spark_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../../data/train-*.csv\", nullValue=\"\\\\N\")\n",
    ")\n",
    "# Drop the first column\n",
    "train_spark_df = train_spark_df.drop(\"_c0\")\n",
    "train_spark_df.describe().show()\n",
    "# Print the dataframe\n",
    "train_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movie</th><th>director</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;tt0003740&quot;</td><td>&quot;nm0665163&quot;</td></tr><tr><td>&quot;tt0008663&quot;</td><td>&quot;nm0803705&quot;</td></tr><tr><td>&quot;tt0009369&quot;</td><td>&quot;nm0428059&quot;</td></tr><tr><td>&quot;tt0009369&quot;</td><td>&quot;nm0949648&quot;</td></tr><tr><td>&quot;tt0010307&quot;</td><td>&quot;nm0304098&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────┬───────────┐\n",
       "│ movie     ┆ director  │\n",
       "│ ---       ┆ ---       │\n",
       "│ str       ┆ str       │\n",
       "╞═══════════╪═══════════╡\n",
       "│ tt0003740 ┆ nm0665163 │\n",
       "│ tt0008663 ┆ nm0803705 │\n",
       "│ tt0009369 ┆ nm0428059 │\n",
       "│ tt0009369 ┆ nm0949648 │\n",
       "│ tt0010307 ┆ nm0304098 │\n",
       "└───────────┴───────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Polars to retrieve the directing data\n",
    "# Load and parse the JSON file\n",
    "with open(\"../../data/directing.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "movies_polars_df = pl.from_dict(data[\"movie\"]).transpose().rename({\"column_0\": \"movie\"})\n",
    "directors_polars_df = (\n",
    "    pl.from_dict(data[\"director\"]).transpose().rename({\"column_0\": \"director\"})\n",
    ")\n",
    "directing_polars_df = pl.concat(\n",
    "    [\n",
    "        movies_polars_df,\n",
    "        directors_polars_df,\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")\n",
    "directing_polars_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|    movie|   writer|\n",
      "+---------+---------+\n",
      "|tt0003740|nm0195339|\n",
      "|tt0003740|nm0515385|\n",
      "|tt0003740|nm0665163|\n",
      "|tt0003740|nm0758215|\n",
      "|tt0008663|nm0406585|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/writing.json\") as f:\n",
    "    data = json.load(f)\n",
    "writing_json = spark.sparkContext.parallelize(data)\n",
    "writing_spark_df = spark.read.json(writing_json)\n",
    "writing_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into DuckDB Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: imdb_train WITH 7959 ROWS!\n",
      "shape: (5, 8)\n",
      "┌───────────┬───────────────┬──────────────┬───────────┬─────────┬──────────────┬──────────┬───────┐\n",
      "│ tconst    ┆ primaryTitle  ┆ originalTitl ┆ startYear ┆ endYear ┆ runtimeMinut ┆ numVotes ┆ label │\n",
      "│ ---       ┆ ---           ┆ e            ┆ ---       ┆ ---     ┆ es           ┆ ---      ┆ ---   │\n",
      "│ str       ┆ str           ┆ ---          ┆ i32       ┆ i32     ┆ ---          ┆ f64      ┆ bool  │\n",
      "│           ┆               ┆ str          ┆           ┆         ┆ i32          ┆          ┆       │\n",
      "╞═══════════╪═══════════════╪══════════════╪═══════════╪═════════╪══════════════╪══════════╪═══════╡\n",
      "│ tt0014109 ┆ The Saga of   ┆ null         ┆ 1924      ┆ null    ┆ 183          ┆ 1231.0   ┆ true  │\n",
      "│           ┆ Gösta Berling ┆              ┆           ┆         ┆              ┆          ┆       │\n",
      "│ tt0015064 ┆ The Last      ┆ Der letzte   ┆ 1924      ┆ null    ┆ 77           ┆ null     ┆ true  │\n",
      "│           ┆ Laugh         ┆ Mann         ┆           ┆         ┆              ┆          ┆       │\n",
      "│ tt0015841 ┆ The Freshman  ┆ The Freshman ┆ 1925      ┆ null    ┆ 77           ┆ 5374.0   ┆ true  │\n",
      "│ tt0017271 ┆ By the Law    ┆ null         ┆ null      ┆ 1926    ┆ 80           ┆ 1057.0   ┆ true  │\n",
      "│ tt0018451 ┆ The Student   ┆ null         ┆ 1927      ┆ null    ┆ 106          ┆ 1459.0   ┆ true  │\n",
      "│           ┆ Prince in Old ┆              ┆           ┆         ┆              ┆          ┆       │\n",
      "│           ┆ Heidel…       ┆              ┆           ┆         ┆              ┆          ┆       │\n",
      "└───────────┴───────────────┴──────────────┴───────────┴─────────┴──────────────┴──────────┴───────┘\n",
      "CREATED TABLE: imdb_directors WITH 11162 ROWS!\n",
      "shape: (5, 2)\n",
      "┌───────────┬───────────┐\n",
      "│ movie     ┆ director  │\n",
      "│ ---       ┆ ---       │\n",
      "│ str       ┆ str       │\n",
      "╞═══════════╪═══════════╡\n",
      "│ tt0003740 ┆ nm0665163 │\n",
      "│ tt0008663 ┆ nm0803705 │\n",
      "│ tt0009369 ┆ nm0428059 │\n",
      "│ tt0009369 ┆ nm0949648 │\n",
      "│ tt0010307 ┆ nm0304098 │\n",
      "└───────────┴───────────┘\n",
      "CREATED TABLE: imdb_writing WITH 22428 ROWS!\n",
      "shape: (5, 2)\n",
      "┌───────────┬───────────┐\n",
      "│ movie     ┆ writer    │\n",
      "│ ---       ┆ ---       │\n",
      "│ str       ┆ str       │\n",
      "╞═══════════╪═══════════╡\n",
      "│ tt0003740 ┆ nm0195339 │\n",
      "│ tt0003740 ┆ nm0515385 │\n",
      "│ tt0003740 ┆ nm0665163 │\n",
      "│ tt0003740 ┆ nm0758215 │\n",
      "│ tt0008663 ┆ nm0406585 │\n",
      "└───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# DuckDBContext to add pyspark tables to DuckDB\n",
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    ctx.save_to_duckdb(train_spark_df, \"imdb_train\")\n",
    "    ctx.show_n(\"imdb_train\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(directing_polars_df, \"imdb_directors\")\n",
    "    ctx.show_n(\"imdb_directors\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(writing_spark_df, \"imdb_writing\")\n",
    "    ctx.show_n(\"imdb_writing\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_name(file_name: str) -> str:\n",
    "    return file_name.rsplit(\".\", 2)[0]\n",
    "\n",
    "\n",
    "def create_url(endpoint: str) -> str:\n",
    "    \"\"\"\n",
    "    Create Url\n",
    "\n",
    "    :param str endpoint: download endpoint\n",
    "    :return str: full url\n",
    "    \"\"\"\n",
    "    return f\"https://datasets.imdbws.com/{endpoint}\"\n",
    "\n",
    "\n",
    "def download_file(url, filename):\n",
    "    print(f\"Downloading file: {filename}\")\n",
    "\n",
    "    response = urllib.request.urlopen(url)\n",
    "\n",
    "    # Get the total file size\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    progress = tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=filename)\n",
    "\n",
    "    chunk_size = 1024  # you can change this to larger if you want\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        while True:\n",
    "            chunk = response.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "            progress.update(len(chunk))\n",
    "    progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dowloading\n",
    "\n",
    "Run this cell once, otherwise you'll keep downloading the same files over and over...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/name.basics.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/name.basics.tsv.gz: 100%|██████████| 263M/263M [00:26<00:00, 9.82MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.akas.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.akas.tsv.gz: 100%|██████████| 330M/330M [00:32<00:00, 10.0MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.basics.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.basics.tsv.gz: 100%|██████████| 186M/186M [00:18<00:00, 9.86MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.crew.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.crew.tsv.gz: 100%|██████████| 70.7M/70.7M [00:07<00:00, 9.68MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.principals.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.principals.tsv.gz:   5%|▍         | 23.3M/471M [00:02<00:40, 11.1MiB/s]"
     ]
    }
   ],
   "source": [
    "extra_imdb = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    # \"title.episode.tsv.gz\", # we have only movie data\n",
    "    \"title.principals.tsv.gz\",\n",
    "    \"title.ratings.tsv.gz\",\n",
    "]\n",
    "\n",
    "# RUN THIS ONCE!\n",
    "# Download the files\n",
    "for ds in extra_imdb:\n",
    "    # Create an instance of the IMDB class with the desired endpoint\n",
    "    download_url = create_url(ds)\n",
    "\n",
    "    filepath = f\"../../data/extra/{ds}\"  # Local fp\n",
    "    # Use the function to download the file\n",
    "    download_file(download_url, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading with Spark\n",
    "\n",
    "RUN THIS ONCE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    train_ids = ctx.conn.execute(\"SELECT tconst FROM imdb_train\").fetchdf()\n",
    "    train_ids_spark = spark.createDataFrame(train_ids)\n",
    "\n",
    "    for ds in extra_imdb:\n",
    "        table_name = f\"extra.{get_table_name(ds)}\".replace(\".\", \"_\")\n",
    "\n",
    "        # Load a small subset of the data to infer the schema\n",
    "        subset = spark.read.csv(\n",
    "            f\"../../data/extra/{ds}\",\n",
    "            header=True,\n",
    "            sep=\"\\t\",\n",
    "            nullValue=\"\\\\N\",\n",
    "            inferSchema=True,\n",
    "        ).limit(1000)\n",
    "\n",
    "        # Extract the schema from the subset\n",
    "        schema = subset.schema\n",
    "\n",
    "        # Load all TSV.GZ files in the data directory into a dataframe with the inferred schema\n",
    "        spark_df = spark.read.csv(\n",
    "            f\"../../data/extra/{ds}\",\n",
    "            header=True,\n",
    "            sep=\"\\t\",\n",
    "            nullValue=\"\\\\N\",\n",
    "            schema=schema,\n",
    "        )\n",
    "        spark_df.show(5)\n",
    "\n",
    "        spark_df_columns = spark_df.columns\n",
    "\n",
    "        if \"titleId\" in spark_df_columns:\n",
    "            filtered_spark_df = spark_df.join(\n",
    "                train_ids_spark, train_ids_spark.tconst == spark_df.titleId, \"inner\"\n",
    "            )\n",
    "            filtered_spark_df = filtered_spark_df.drop(\"titleId\")\n",
    "        elif \"knownForTitles\" in spark_df_columns:\n",
    "            # Split the knownForTitles column into multiple rows\n",
    "            spark_df = spark_df.withColumn(\n",
    "                \"knownForTitles\", explode(split(spark_df[\"knownForTitles\"], \",\"))\n",
    "            )\n",
    "\n",
    "            # Select the values that are in both train_ids_spark and spark_df\n",
    "            filtered_spark_df = spark_df.join(\n",
    "                train_ids_spark,\n",
    "                spark_df.knownForTitles == train_ids_spark.tconst,\n",
    "                \"inner\",\n",
    "            )\n",
    "        elif \"tconst\" in spark_df_columns:\n",
    "            filtered_spark_df = spark_df.join(train_ids_spark, \"tconst\", \"inner\")\n",
    "\n",
    "        ctx.save_to_duckdb(filtered_spark_df, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert strings to lists\n",
    "def parse_list(s):\n",
    "    return s.strip(\"[]\").split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letterboxd Movie Ratings Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/samlearner/letterboxd-movie-ratings-data/download?datasetVersionNumber=6\",\n",
    "    data_dir=\"../../data/extra\",\n",
    ")\n",
    "# Oscar Award Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/unanimad/the-oscar-award\",\n",
    "    data_dir=\"../../data/extra\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all CSV files\n",
    "for file_path in csv_files:\n",
    "    # Check if 'users_export.csv' or 'ratings_export.csv' is part of the file name\n",
    "    if \"users_export.csv\" in file_path or \"ratings_export.csv\" in file_path:\n",
    "        # If the file exists, remove it\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"{file_path} removed successfully.\")\n",
    "        else:\n",
    "            print(f\"{file_path} does not exist.\")\n",
    "    else:\n",
    "        df = (\n",
    "            spark.read.csv(\n",
    "                file_path,\n",
    "                header=True,\n",
    "                inferSchema=True,\n",
    "            )\n",
    "            .limit(5)\n",
    "            .show(5)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert strings to lists\n",
    "def parse_list(s):\n",
    "    return s.strip(\"[]\").split(\", \")\n",
    "\n",
    "\n",
    "parse_list_udf = udf(parse_list, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def process_column(df, column_name):\n",
    "    # Remove the extra double quotes\n",
    "    df = df.withColumn(column_name, regexp_replace(df[column_name], '\"', \"\"))\n",
    "    print(f\"Size after removing double quotes: {df.count()} rows\")\n",
    "\n",
    "    # Filter the DataFrame to only include rows where the column is not null\n",
    "    df = df.filter(col(column_name).isNotNull())\n",
    "    print(f\"Size after filtering nulls: {df.count()} rows\")\n",
    "\n",
    "    # Convert the column to a list\n",
    "    df = df.withColumn(column_name, parse_list_udf(df[column_name]))\n",
    "    print(f\"Size after converting to list: {df.count()} rows\")\n",
    "\n",
    "    # Check if there are any arrays with multiple values\n",
    "    multi_value_rows = df.filter(size(df[column_name]) > 1)\n",
    "    print(\n",
    "        f\"Number of rows with multiple values in {column_name}: {multi_value_rows.count()}\"\n",
    "    )\n",
    "\n",
    "    # Explode the array into new rows\n",
    "    df = df.withColumn(column_name[:-1], explode(df[column_name]))\n",
    "    print(f\"Size after exploding list: {df.count()} rows\")\n",
    "\n",
    "    # Drop the original column\n",
    "    df = df.drop(column_name)\n",
    "    print(f\"Size after dropping original column: {df.count()} rows\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DuckDBContext\n",
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    train_ids = ctx.conn.execute(\"SELECT tconst FROM imdb_train\").fetchdf()\n",
    "    train_ids_spark = spark.createDataFrame(train_ids)\n",
    "\n",
    "    spark_df = spark.read.csv(\n",
    "        \"../../data/extra/the-oscar-award/the_oscar_award.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    # Save the DataFrame to DuckDB\n",
    "    ctx.save_to_duckdb(spark_df, \"the_oscar_award\")\n",
    "\n",
    "    # Read Movie Data & drop unnecessary columns\n",
    "    spark_df = spark.read.csv(\n",
    "        \"../../data/extra/letterboxd-movie-ratings-data/movie_data.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    ).drop(\"image_url\", \"imdb_link\", \"overview\", \"tmdb_id\", \"tmdb_link\")\n",
    "\n",
    "    # Filter on our Train IDs\n",
    "    filtered_spark_df = spark_df.join(\n",
    "        train_ids_spark, spark_df.imdb_id == train_ids_spark.tconst, \"inner\"\n",
    "    ).drop(\"imdb_id\")\n",
    "    filtered_spark_df.show(5)\n",
    "\n",
    "    # Process the genres, production_countries, and spoken_languages columns\n",
    "    for column in [\"genres\", \"production_countries\", \"spoken_languages\"]:\n",
    "        filtered_spark_df = process_column(filtered_spark_df, column)\n",
    "\n",
    "    filtered_spark_df.show(5)\n",
    "\n",
    "    # Save the DataFrame to DuckDB\n",
    "    ctx.save_to_duckdb(filtered_spark_df, \"letterboxd_movie_ratings_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
