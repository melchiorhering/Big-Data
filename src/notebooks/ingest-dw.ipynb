{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data Warehouse\n",
    "\n",
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import Optional\n",
    "from lib.duckdbcontext import DuckDBContext\n",
    "import polars as pl\n",
    "import pyspark\n",
    "import opendatasets as od\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    explode,\n",
    "    split,\n",
    "    udf,\n",
    "    size,\n",
    "    regexp_replace,\n",
    "    when,\n",
    "    array,\n",
    "    countDistinct,\n",
    "    col,\n",
    ")\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import ast\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "import duckdb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "0.9.2\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)\n",
    "print(duckdb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_database = \"../orchestration/db/bigdata.duckdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Cluster Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 16:49:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Connect to Existing Spark Cluster\n",
    "# spark = (\n",
    "#     SparkSession.builder.master(\"spark://spark:7077\")\n",
    "#     .appName(\"Spark-ETL\")\n",
    "#     .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# Connect to local Spark Sessions\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local\").appName(\"Spark-ETL\")\n",
    "    # .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to Data Warehouse\n",
    "\n",
    "## Initial Data\n",
    "\n",
    "#### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/train-8.csv', '../../data/train-2.csv', '../../data/train-7.csv', '../../data/train-5.csv', '../../data/train-3.csv', '../../data/train-4.csv', '../../data/train-1.csv', '../../data/train-6.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:49:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|   tconst|        primaryTitle|     originalTitle|         startYear|           endYear|    runtimeMinutes|          numVotes|\n",
      "+-------+---------+--------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|     7959|                7959|              3971|              7173|               786|              7946|              7169|\n",
      "|   mean|     NULL|   1231.388888888889|            1128.0|1997.9960964728843|1998.7633587786258|105.68713818273345| 29520.51081043381|\n",
      "| stddev|     NULL|   954.6947857755001|1038.0438333712118| 21.99534723241901|   21.895931761063| 25.39634772412447|114449.99384975343|\n",
      "|    min|tt0009369|\"Drágớn Báll Z: R...|     'A' gai wak 2|              1918|              1921|                45|            1001.0|\n",
      "|    max|tt9911196|             Ớútcást|        Üç Kagitçi|              2021|              2021|               551|         2503641.0|\n",
      "+-------+---------+--------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n",
      "+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "|   tconst|        primaryTitle|  originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "|tt0014109|The Saga of Gösta...|           NULL|     1924|   NULL|           183|  1231.0| true|\n",
      "|tt0015064|      The Last Laugh|Der letzte Mann|     1924|   NULL|            77|    NULL| true|\n",
      "|tt0015841|        The Freshman|   The Freshman|     1925|   NULL|            77|  5374.0| true|\n",
      "|tt0017271|          By the Law|           NULL|     NULL|   1926|            80|  1057.0| true|\n",
      "|tt0018451|The Student Princ...|           NULL|     1927|   NULL|           106|  1459.0| true|\n",
      "+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all CSV files that match the pattern\n",
    "csv_files = glob.glob(\"../../data/train-*.csv\")\n",
    "print(csv_files)\n",
    "\n",
    "# Load all CSV files in the data directory into a dataframe\n",
    "# Specify '\\\\N' as a null value\n",
    "# Ignore the header and infer the schema from data\n",
    "train_spark_df = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../../data/train-*.csv\", nullValue=\"\\\\N\")\n",
    ")\n",
    "# Drop the first column\n",
    "train_spark_df = train_spark_df.drop(\"_c0\")\n",
    "train_spark_df.describe().show()\n",
    "# Print the dataframe\n",
    "train_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>movie</th><th>director</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;tt0003740&quot;</td><td>&quot;nm0665163&quot;</td></tr><tr><td>&quot;tt0008663&quot;</td><td>&quot;nm0803705&quot;</td></tr><tr><td>&quot;tt0009369&quot;</td><td>&quot;nm0428059&quot;</td></tr><tr><td>&quot;tt0009369&quot;</td><td>&quot;nm0949648&quot;</td></tr><tr><td>&quot;tt0010307&quot;</td><td>&quot;nm0304098&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────┬───────────┐\n",
       "│ movie     ┆ director  │\n",
       "│ ---       ┆ ---       │\n",
       "│ str       ┆ str       │\n",
       "╞═══════════╪═══════════╡\n",
       "│ tt0003740 ┆ nm0665163 │\n",
       "│ tt0008663 ┆ nm0803705 │\n",
       "│ tt0009369 ┆ nm0428059 │\n",
       "│ tt0009369 ┆ nm0949648 │\n",
       "│ tt0010307 ┆ nm0304098 │\n",
       "└───────────┴───────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Polars to retrieve the directing data\n",
    "# Load and parse the JSON file\n",
    "with open(\"../../data/directing.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "movies_polars_df = pl.from_dict(data[\"movie\"]).transpose().rename({\"column_0\": \"movie\"})\n",
    "directors_polars_df = (\n",
    "    pl.from_dict(data[\"director\"]).transpose().rename({\"column_0\": \"director\"})\n",
    ")\n",
    "directing_polars_df = pl.concat(\n",
    "    [\n",
    "        movies_polars_df,\n",
    "        directors_polars_df,\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")\n",
    "directing_polars_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|    movie|   writer|\n",
      "+---------+---------+\n",
      "|tt0003740|nm0195339|\n",
      "|tt0003740|nm0515385|\n",
      "|tt0003740|nm0665163|\n",
      "|tt0003740|nm0758215|\n",
      "|tt0008663|nm0406585|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/writing.json\") as f:\n",
    "    data = json.load(f)\n",
    "writing_json = spark.sparkContext.parallelize(data)\n",
    "writing_spark_df = spark.read.json(writing_json)\n",
    "writing_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into DuckDB Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: imdb_train WITH 7959 ROWS!\n",
      "shape: (5, 8)\n",
      "┌───────────┬───────────────┬──────────────┬───────────┬─────────┬──────────────┬──────────┬───────┐\n",
      "│ tconst    ┆ primaryTitle  ┆ originalTitl ┆ startYear ┆ endYear ┆ runtimeMinut ┆ numVotes ┆ label │\n",
      "│ ---       ┆ ---           ┆ e            ┆ ---       ┆ ---     ┆ es           ┆ ---      ┆ ---   │\n",
      "│ str       ┆ str           ┆ ---          ┆ i32       ┆ i32     ┆ ---          ┆ f64      ┆ bool  │\n",
      "│           ┆               ┆ str          ┆           ┆         ┆ i32          ┆          ┆       │\n",
      "╞═══════════╪═══════════════╪══════════════╪═══════════╪═════════╪══════════════╪══════════╪═══════╡\n",
      "│ tt0014109 ┆ The Saga of   ┆ null         ┆ 1924      ┆ null    ┆ 183          ┆ 1231.0   ┆ true  │\n",
      "│           ┆ Gösta Berling ┆              ┆           ┆         ┆              ┆          ┆       │\n",
      "│ tt0015064 ┆ The Last      ┆ Der letzte   ┆ 1924      ┆ null    ┆ 77           ┆ null     ┆ true  │\n",
      "│           ┆ Laugh         ┆ Mann         ┆           ┆         ┆              ┆          ┆       │\n",
      "│ tt0015841 ┆ The Freshman  ┆ The Freshman ┆ 1925      ┆ null    ┆ 77           ┆ 5374.0   ┆ true  │\n",
      "│ tt0017271 ┆ By the Law    ┆ null         ┆ null      ┆ 1926    ┆ 80           ┆ 1057.0   ┆ true  │\n",
      "│ tt0018451 ┆ The Student   ┆ null         ┆ 1927      ┆ null    ┆ 106          ┆ 1459.0   ┆ true  │\n",
      "│           ┆ Prince in Old ┆              ┆           ┆         ┆              ┆          ┆       │\n",
      "│           ┆ Heidel…       ┆              ┆           ┆         ┆              ┆          ┆       │\n",
      "└───────────┴───────────────┴──────────────┴───────────┴─────────┴──────────────┴──────────┴───────┘\n",
      "CREATED TABLE: imdb_directors WITH 11162 ROWS!\n",
      "shape: (5, 2)\n",
      "┌───────────┬───────────┐\n",
      "│ movie     ┆ director  │\n",
      "│ ---       ┆ ---       │\n",
      "│ str       ┆ str       │\n",
      "╞═══════════╪═══════════╡\n",
      "│ tt0003740 ┆ nm0665163 │\n",
      "│ tt0008663 ┆ nm0803705 │\n",
      "│ tt0009369 ┆ nm0428059 │\n",
      "│ tt0009369 ┆ nm0949648 │\n",
      "│ tt0010307 ┆ nm0304098 │\n",
      "└───────────┴───────────┘\n",
      "CREATED TABLE: imdb_writing WITH 22428 ROWS!\n",
      "shape: (5, 2)\n",
      "┌───────────┬───────────┐\n",
      "│ movie     ┆ writer    │\n",
      "│ ---       ┆ ---       │\n",
      "│ str       ┆ str       │\n",
      "╞═══════════╪═══════════╡\n",
      "│ tt0003740 ┆ nm0195339 │\n",
      "│ tt0003740 ┆ nm0515385 │\n",
      "│ tt0003740 ┆ nm0665163 │\n",
      "│ tt0003740 ┆ nm0758215 │\n",
      "│ tt0008663 ┆ nm0406585 │\n",
      "└───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# DuckDBContext to add pyspark tables to DuckDB\n",
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    ctx.save_to_duckdb(train_spark_df, \"imdb_train\")\n",
    "    ctx.show_n(\"imdb_train\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(directing_polars_df, \"imdb_directors\")\n",
    "    ctx.show_n(\"imdb_directors\", 5)\n",
    "\n",
    "    ctx.save_to_duckdb(writing_spark_df, \"imdb_writing\")\n",
    "    ctx.show_n(\"imdb_writing\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_name(file_name: str) -> str:\n",
    "    return file_name.rsplit(\".\", 2)[0]\n",
    "\n",
    "\n",
    "def create_url(endpoint: str) -> str:\n",
    "    \"\"\"\n",
    "    Create Url\n",
    "\n",
    "    :param str endpoint: download endpoint\n",
    "    :return str: full url\n",
    "    \"\"\"\n",
    "    return f\"https://datasets.imdbws.com/{endpoint}\"\n",
    "\n",
    "\n",
    "def download_file(url, filename):\n",
    "    print(f\"Downloading file: {filename}\")\n",
    "\n",
    "    response = urllib.request.urlopen(url)\n",
    "\n",
    "    # Get the total file size\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Create a tqdm progress bar\n",
    "    progress = tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=filename)\n",
    "\n",
    "    chunk_size = 1024  # you can change this to larger if you want\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        while True:\n",
    "            chunk = response.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f.write(chunk)\n",
    "            progress.update(len(chunk))\n",
    "    progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dowloading\n",
    "\n",
    "Run this cell once, otherwise you'll keep downloading the same files over and over...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/name.basics.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/name.basics.tsv.gz: 100%|██████████| 263M/263M [00:26<00:00, 9.82MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.akas.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.akas.tsv.gz: 100%|██████████| 330M/330M [00:32<00:00, 10.0MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.basics.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.basics.tsv.gz: 100%|██████████| 186M/186M [00:18<00:00, 9.86MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.crew.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.crew.tsv.gz: 100%|██████████| 70.7M/70.7M [00:07<00:00, 9.68MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.principals.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.principals.tsv.gz: 100%|██████████| 471M/471M [00:50<00:00, 9.25MiB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ../../data/extra/title.ratings.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../data/extra/title.ratings.tsv.gz: 100%|██████████| 7.11M/7.11M [00:00<00:00, 8.41MiB/s]\n"
     ]
    }
   ],
   "source": [
    "extra_imdb = [\n",
    "    \"name.basics.tsv.gz\",\n",
    "    \"title.akas.tsv.gz\",\n",
    "    \"title.basics.tsv.gz\",\n",
    "    \"title.crew.tsv.gz\",\n",
    "    # \"title.episode.tsv.gz\", # we have only movie data\n",
    "    \"title.principals.tsv.gz\",\n",
    "    \"title.ratings.tsv.gz\",\n",
    "]\n",
    "\n",
    "# RUN THIS ONCE!\n",
    "# Download the files\n",
    "for ds in extra_imdb:\n",
    "    # Create an instance of the IMDB class with the desired endpoint\n",
    "    download_url = create_url(ds)\n",
    "\n",
    "    filepath = f\"../../data/extra/{ds}\"  # Local fp\n",
    "    # Use the function to download the file\n",
    "    download_file(download_url, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading with Spark\n",
    "\n",
    "RUN THIS ONCE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|    primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|nm0000001|   Fred Astaire|     1899|     1987|soundtrack,actor,...|tt0072308,tt00531...|\n",
      "|nm0000002|  Lauren Bacall|     1924|     2014|  actress,soundtrack|tt0038355,tt01170...|\n",
      "|nm0000003|Brigitte Bardot|     1934|     NULL|actress,soundtrac...|tt0056404,tt00573...|\n",
      "|nm0000004|   John Belushi|     1949|     1982|actor,soundtrack,...|tt0072562,tt00779...|\n",
      "|nm0000005| Ingmar Bergman|     1918|     2007|writer,director,a...|tt0083922,tt00694...|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: extra_name_basics WITH 555345 ROWS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|  titleId|ordering|               title|region|language|      types|   attributes|isOriginalTitle|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "|tt0000001|       1|          Карменсіта|    UA|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       2|          Carmencita|    DE|    NULL|       NULL|literal title|              0|\n",
      "|tt0000001|       3|Carmencita - span...|    HU|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       4|          Καρμενσίτα|    GR|    NULL|imdbDisplay|         NULL|              0|\n",
      "|tt0000001|       5|          Карменсита|    RU|    NULL|imdbDisplay|         NULL|              0|\n",
      "+---------+--------+--------------------+------+--------+-----------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: extra_title_akas WITH 190567 ROWS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|   NULL|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|   NULL|             5|     Animation,Short|\n",
      "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|   NULL|             4|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|   NULL|            12|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|   NULL|             1|        Comedy,Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: extra_title_basics WITH 7958 ROWS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "|   tconst|directors|writers|\n",
      "+---------+---------+-------+\n",
      "|tt0000001|nm0005690|   NULL|\n",
      "|tt0000002|nm0721526|   NULL|\n",
      "|tt0000003|nm0721526|   NULL|\n",
      "|tt0000004|nm0721526|   NULL|\n",
      "|tt0000005|nm0005690|   NULL|\n",
      "+---------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: extra_title_crew WITH 7958 ROWS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|   tconst|ordering|   nconst|       category|                 job|characters|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "|tt0000001|       1|nm1588970|           self|                NULL|  [\"Self\"]|\n",
      "|tt0000001|       2|nm0005690|       director|                NULL|      NULL|\n",
      "|tt0000001|       3|nm0374658|cinematographer|director of photo...|      NULL|\n",
      "|tt0000002|       1|nm0721526|       director|                NULL|      NULL|\n",
      "|tt0000002|       2|nm1335271|       composer|                NULL|      NULL|\n",
      "+---------+--------+---------+---------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: extra_title_principals WITH 77941 ROWS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+--------+\n",
      "|   tconst|averageRating|numVotes|\n",
      "+---------+-------------+--------+\n",
      "|tt0000001|          5.7|    2035|\n",
      "|tt0000002|          5.7|     272|\n",
      "|tt0000003|          6.5|    1982|\n",
      "|tt0000004|          5.4|     178|\n",
      "|tt0000005|          6.2|    2739|\n",
      "+---------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: extra_title_ratings WITH 7958 ROWS!\n"
     ]
    }
   ],
   "source": [
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    train_ids = ctx.conn.execute(\"SELECT tconst FROM imdb_train\").fetchdf()\n",
    "    train_ids_spark = spark.createDataFrame(train_ids)\n",
    "\n",
    "    for ds in extra_imdb:\n",
    "        table_name = f\"extra.{get_table_name(ds)}\".replace(\".\", \"_\")\n",
    "\n",
    "        # Load a small subset of the data to infer the schema\n",
    "        subset = spark.read.csv(\n",
    "            f\"../../data/extra/{ds}\",\n",
    "            header=True,\n",
    "            sep=\"\\t\",\n",
    "            nullValue=\"\\\\N\",\n",
    "            inferSchema=True,\n",
    "        ).limit(1000)\n",
    "\n",
    "        # Extract the schema from the subset\n",
    "        schema = subset.schema\n",
    "\n",
    "        # Load all TSV.GZ files in the data directory into a dataframe with the inferred schema\n",
    "        spark_df = spark.read.csv(\n",
    "            f\"../../data/extra/{ds}\",\n",
    "            header=True,\n",
    "            sep=\"\\t\",\n",
    "            nullValue=\"\\\\N\",\n",
    "            schema=schema,\n",
    "        )\n",
    "        spark_df.show(5)\n",
    "\n",
    "        spark_df_columns = spark_df.columns\n",
    "\n",
    "        if \"titleId\" in spark_df_columns:\n",
    "            filtered_spark_df = spark_df.join(\n",
    "                train_ids_spark, train_ids_spark.tconst == spark_df.titleId, \"inner\"\n",
    "            )\n",
    "            filtered_spark_df = filtered_spark_df.drop(\"titleId\")\n",
    "        elif \"knownForTitles\" in spark_df_columns:\n",
    "            # Split the knownForTitles column into multiple rows\n",
    "            spark_df = spark_df.withColumn(\n",
    "                \"knownForTitles\", explode(split(spark_df[\"knownForTitles\"], \",\"))\n",
    "            )\n",
    "\n",
    "            # Select the values that are in both train_ids_spark and spark_df\n",
    "            filtered_spark_df = spark_df.join(\n",
    "                train_ids_spark,\n",
    "                spark_df.knownForTitles == train_ids_spark.tconst,\n",
    "                \"inner\",\n",
    "            )\n",
    "        elif \"tconst\" in spark_df_columns:\n",
    "            filtered_spark_df = spark_df.join(train_ids_spark, \"tconst\", \"inner\")\n",
    "\n",
    "        ctx.save_to_duckdb(filtered_spark_df, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert strings to lists\n",
    "def parse_list(s):\n",
    "    return s.strip(\"[]\").split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"../../data/extra/letterboxd-movie-ratings-data\" (use force=True to force download)\n",
      "Skipping, found downloaded files in \"../../data/extra/the-oscar-award\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# Letterboxd Movie Ratings Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/samlearner/letterboxd-movie-ratings-data/download?datasetVersionNumber=6\",\n",
    "    data_dir=\"../../data/extra\",\n",
    ")\n",
    "# Oscar Award Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/unanimad/the-oscar-award\",\n",
    "    data_dir=\"../../data/extra\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:59:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-8.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+------------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|        primaryTitle|     originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+--------------------+------------------+---------+-------+--------------+--------+-----+\n",
      "| 29|tt0015224|           Peter Pan|              NULL|     1924|     \\N|           105|  1042.0| true|\n",
      "| 35|tt0015864|       The Gold Rush|              NULL|     1925|     \\N|            95|107475.0| true|\n",
      "| 37|tt0016029|  The Little Colonel|              NULL|     1935|     \\N|            81|  1646.0| true|\n",
      "| 82|tt0021309|The Story of the Fox|Le roman de Renard|     1937|     \\N|            63|    NULL| true|\n",
      "| 93|tt0022395|       The Skin Game|              NULL|     1931|     \\N|            85|    NULL|false|\n",
      "+---+---------+--------------------+------------------+---------+-------+--------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:59:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-2.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------------------+-------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|      primaryTitle|originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+------------------+-------------+---------+-------+--------------+--------+-----+\n",
      "|  6|tt0011607|The Parson's Widow|   Prästänkan|     1920|     \\N|            94|  1264.0| true|\n",
      "| 17|tt0014358|       The Pilgrim|         NULL|     1923|     \\N|            47|  4891.0| true|\n",
      "| 20|tt0014611|        Why Worry?|         NULL|     1923|     \\N|            63|  1739.0| true|\n",
      "| 44|tt0016847|             Faust|         NULL|     1926|     \\N|           107| 14809.0| true|\n",
      "| 63|tt0018773|        Thé Circús|         NULL|     1928|     \\N|            72| 32601.0| true|\n",
      "+---+---------+------------------+-------------+---------+-------+--------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:59:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-7.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+-------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|        primaryTitle|originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+--------------------+-------------+---------+-------+--------------+--------+-----+\n",
      "|  2|tt0009369|              Mickey|       Mickey|     1918|     \\N|            93|  1119.0|false|\n",
      "| 15|tt0014142|The Hunchback of ...|         NULL|       \\N|   1923|           133|  5288.0| true|\n",
      "| 21|tt0014945|            Girl Shy|     Girl Shy|     1924|     \\N|            87|  3327.0| true|\n",
      "| 45|tt0017048|   A Page of Madness|         NULL|     1926|     \\N|            70|  3357.0| true|\n",
      "| 48|tt0017350|  The Scarlet Letter|         NULL|     1926|     \\N|           115|  1768.0| true|\n",
      "+---+---------+--------------------+-------------+---------+-------+--------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:59:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-5.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|        primaryTitle|       originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "|  8|tt0012349|             The Kid|                NULL|     1921|     \\N|            68|121452.0| true|\n",
      "| 30|tt0015361|              Strike|                NULL|     1925|     \\N|            82|  7695.0| true|\n",
      "| 36|tt0015881|               Greed|                NULL|     1924|     \\N|           140|  9649.0| true|\n",
      "| 53|tt0018066|Thé Énd ớf St. Pé...|Konets Sankt-Pete...|     1927|     \\N|            85|    NULL| true|\n",
      "| 70|tt0019412|              Speedy|                NULL|     1928|     \\N|            85|  3613.0| true|\n",
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:59:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-3.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+-----------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|        primaryTitle|    originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+--------------------+-----------------+---------+-------+--------------+--------+-----+\n",
      "|  5|tt0011439|   The Mark of Zorro|The Mark of Zorro|     1920|     \\N|            79|  2439.0| true|\n",
      "| 10|tt0012532|Ớrpháns ớf thé Stớrm|             NULL|     1921|     \\N|           150|    NULL| true|\n",
      "| 13|tt0013933|  The Faithful Heart|     Coeur fidèle|     1923|     \\N|            87|  1252.0| true|\n",
      "| 31|tt0015400| The Thief of Bagdad|             NULL|     1924|     \\N|           155|  6001.0| true|\n",
      "| 33|tt0015842|  The Joyless Street|             NULL|     1925|     \\N|           125|  1554.0| true|\n",
      "+---+---------+--------------------+-----------------+---------+-------+--------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:59:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-4.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|        primaryTitle|  originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "| 14|tt0014109|The Saga of Gösta...|           NULL|     1924|     \\N|           183|  1231.0| true|\n",
      "| 24|tt0015064|      The Last Laugh|Der letzte Mann|     1924|     \\N|            77|    NULL| true|\n",
      "| 32|tt0015841|        The Freshman|   The Freshman|     1925|     \\N|            77|  5374.0| true|\n",
      "| 47|tt0017271|          By the Law|           NULL|       \\N|   1926|            80|  1057.0| true|\n",
      "| 56|tt0018451|The Student Princ...|           NULL|     1927|     \\N|           106|  1459.0| true|\n",
      "+---+---------+--------------------+---------------+---------+-------+--------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 16:59:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-1.csv\n",
      "24/03/17 16:59:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      " Schema: _c0, tconst, primaryTitle, originalTitle, startYear, endYear, runtimeMinutes, numVotes, label\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///workspaces/Big-Data/data/train-6.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|        primaryTitle|       originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "|  4|tt0010600|            The Doll|           Die Puppe|     1919|     \\N|            66|  1898.0| true|\n",
      "|  7|tt0011841|       Way Down East|       Way Down East|     1920|     \\N|           145|  5376.0| true|\n",
      "|  9|tt0012494|             Déstiny|        Der müde Tod|     1921|     \\N|            97|  5842.0| true|\n",
      "| 25|tt0015163|       The Navigator|       The Navigator|     1924|     \\N|            59|  9652.0| true|\n",
      "| 38|tt0016220|The Phantom of th...|The Phantom of th...|     1925|     \\N|            93| 17887.0| true|\n",
      "+---+---------+--------------------+--------------------+---------+-------+--------------+--------+-----+\n",
      "\n",
      "+---+---------+---------------+---------------+---------+-------+--------------+--------+-----+\n",
      "|_c0|   tconst|   primaryTitle|  originalTitle|startYear|endYear|runtimeMinutes|numVotes|label|\n",
      "+---+---------+---------------+---------------+---------+-------+--------------+--------+-----+\n",
      "| 11|tt0013257|          Häxan|          Häxan|     1922|     \\N|            91| 13679.0| true|\n",
      "| 12|tt0013556|     Robin Hood|           NULL|     1922|     \\N|           143|  2178.0| true|\n",
      "| 16|tt0014341|Our Hospitality|Our Hospitality|     1923|     \\N|            65| 10911.0| true|\n",
      "| 19|tt0014538|     Three Ages|           NULL|     1923|     \\N|            63|  4312.0| true|\n",
      "| 50|tt0017925|    The General|           NULL|     1926|     \\N|            67| 87784.0| true|\n",
      "+---+---------+---------------+---------------+---------+-------+--------------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over all CSV files\n",
    "for file_path in csv_files:\n",
    "    # Check if 'users_export.csv' or 'ratings_export.csv' is part of the file name\n",
    "    if \"users_export.csv\" in file_path or \"ratings_export.csv\" in file_path:\n",
    "        # If the file exists, remove it\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"{file_path} removed successfully.\")\n",
    "        else:\n",
    "            print(f\"{file_path} does not exist.\")\n",
    "    else:\n",
    "        df = (\n",
    "            spark.read.csv(\n",
    "                file_path,\n",
    "                header=True,\n",
    "                inferSchema=True,\n",
    "            )\n",
    "            .limit(5)\n",
    "            .show(5)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to convert strings to lists\n",
    "def parse_list(s):\n",
    "    return s.strip(\"[]\").split(\", \")\n",
    "\n",
    "\n",
    "parse_list_udf = udf(parse_list, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "def process_column(df, column_name):\n",
    "    # Remove the extra double quotes\n",
    "    df = df.withColumn(column_name, regexp_replace(df[column_name], '\"', \"\"))\n",
    "    print(f\"Size after removing double quotes: {df.count()} rows\")\n",
    "\n",
    "    # Filter the DataFrame to only include rows where the column is not null\n",
    "    df = df.filter(col(column_name).isNotNull())\n",
    "    print(f\"Size after filtering nulls: {df.count()} rows\")\n",
    "\n",
    "    # Convert the column to a list\n",
    "    df = df.withColumn(column_name, parse_list_udf(df[column_name]))\n",
    "    print(f\"Size after converting to list: {df.count()} rows\")\n",
    "\n",
    "    # Check if there are any arrays with multiple values\n",
    "    multi_value_rows = df.filter(size(df[column_name]) > 1)\n",
    "    print(\n",
    "        f\"Number of rows with multiple values in {column_name}: {multi_value_rows.count()}\"\n",
    "    )\n",
    "\n",
    "    # Explode the array into new rows\n",
    "    df = df.withColumn(column_name[:-1], explode(df[column_name]))\n",
    "    print(f\"Size after exploding list: {df.count()} rows\")\n",
    "\n",
    "    # Drop the original column\n",
    "    df = df.drop(column_name)\n",
    "    print(f\"Size after dropping original column: {df.count()} rows\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: the_oscar_award WITH 10889 ROWS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------------+------------------+-----------------+----------+--------------------+------------+-------+----------------+------------+----------+-------------+---------+\n",
      "|                 _id|             genres|          movie_id|       movie_title|original_language|popularity|production_countries|release_date|runtime|spoken_languages|vote_average|vote_count|year_released|   tconst|\n",
      "+--------------------+-------------------+------------------+------------------+-----------------+----------+--------------------+------------+-------+----------------+------------+----------+-------------+---------+\n",
      "|5fc86a3d6758f6963...|      \"[\"\"Drama\"\"]\"|the-trump-prophecy|The Trump Prophecy|               en|     2.025|\"[\"\"United States...|  2018-10-02|    120| \"[\"\"English\"\"]\"|           4|         7|         2018|tt8235296|\n",
      "|5fc8708a6758f6963...|     \"[\"\"Comedy\"\"]\"|     nothing-funny|     Nothing Funny|               pl|     4.192|      \"[\"\"Poland\"\"]\"|  1996-02-02|     95|  \"[\"\"Polski\"\"]\"|         7.2|        39|         1996|tt0113971|\n",
      "|5fc8738e6758f6963...|\"[\"\"Documentary\"\"]\"|     whose-streets|    Whose Streets?|               en|     2.919|\"[\"\"United States...|  2017-08-11|     90| \"[\"\"English\"\"]\"|         5.8|        19|         2017|tt6176928|\n",
      "|5fc879006758f6963...|      \"[\"\"Drama\"\"]\"|    tunes-of-glory|    Tunes of Glory|               en|     2.941|\"[\"\"United Kingdo...|  1960-09-17|    106| \"[\"\"English\"\"]\"|         7.1|        38|         1960|tt0054412|\n",
      "|5fc87a246758f6963...|     \"[\"\"Action\"\"]\"|          baaghi-2|          Baaghi 2|               hi|     4.897|       \"[\"\"India\"\"]\"|  2018-03-30|    145|  \"[\"\"हिन्दी\"\"]\"|         5.8|        40|         2018|tt6843812|\n",
      "+--------------------+-------------------+------------------+------------------+-----------------+----------+--------------------+------------+-------+----------------+------------+----------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing double quotes: 2268 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after filtering nulls: 2268 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after converting to list: 2268 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with multiple values in genres: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after exploding list: 2268 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after dropping original column: 2268 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing double quotes: 2268 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after filtering nulls: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after converting to list: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with multiple values in production_countries: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after exploding list: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after dropping original column: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing double quotes: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after filtering nulls: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after converting to list: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with multiple values in spoken_languages: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after exploding list: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after dropping original column: 2258 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+-----------------+----------+------------+-------+------------+----------+-------------+---------+-----------+--------------------+---------------+\n",
      "|                 _id|          movie_id|       movie_title|original_language|popularity|release_date|runtime|vote_average|vote_count|year_released|   tconst|      genre| production_countrie|spoken_language|\n",
      "+--------------------+------------------+------------------+-----------------+----------+------------+-------+------------+----------+-------------+---------+-----------+--------------------+---------------+\n",
      "|5fc86a3d6758f6963...|the-trump-prophecy|The Trump Prophecy|               en|     2.025|  2018-10-02|    120|           4|         7|         2018|tt8235296|      Drama|United States of ...|        English|\n",
      "|5fc8708a6758f6963...|     nothing-funny|     Nothing Funny|               pl|     4.192|  1996-02-02|     95|         7.2|        39|         1996|tt0113971|     Comedy|              Poland|         Polski|\n",
      "|5fc8738e6758f6963...|     whose-streets|    Whose Streets?|               en|     2.919|  2017-08-11|     90|         5.8|        19|         2017|tt6176928|Documentary|United States of ...|        English|\n",
      "|5fc879006758f6963...|    tunes-of-glory|    Tunes of Glory|               en|     2.941|  1960-09-17|    106|         7.1|        38|         1960|tt0054412|      Drama|      United Kingdom|        English|\n",
      "|5fc87a246758f6963...|          baaghi-2|          Baaghi 2|               hi|     4.897|  2018-03-30|    145|         5.8|        40|         2018|tt6843812|     Action|               India|         हिन्दी|\n",
      "+--------------------+------------------+------------------+-----------------+----------+------------+-------+------------+----------+-------------+---------+-----------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATED TABLE: letterboxd_movie_ratings_data WITH 2258 ROWS!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DuckDBContext\n",
    "with DuckDBContext(duckdb_database) as ctx:\n",
    "    train_ids = ctx.conn.execute(\"SELECT tconst FROM imdb_train\").fetchdf()\n",
    "    train_ids_spark = spark.createDataFrame(train_ids)\n",
    "\n",
    "    spark_df = spark.read.csv(\n",
    "        \"../../data/extra/the-oscar-award/the_oscar_award.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    # Save the DataFrame to DuckDB\n",
    "    ctx.save_to_duckdb(spark_df, \"the_oscar_award\")\n",
    "\n",
    "    # Read Movie Data & drop unnecessary columns\n",
    "    spark_df = spark.read.csv(\n",
    "        \"../../data/extra/letterboxd-movie-ratings-data/movie_data.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    ).drop(\"image_url\", \"imdb_link\", \"overview\", \"tmdb_id\", \"tmdb_link\")\n",
    "\n",
    "    # Filter on our Train IDs\n",
    "    filtered_spark_df = spark_df.join(\n",
    "        train_ids_spark, spark_df.imdb_id == train_ids_spark.tconst, \"inner\"\n",
    "    ).drop(\"imdb_id\")\n",
    "    filtered_spark_df.show(5)\n",
    "\n",
    "    # Process the genres, production_countries, and spoken_languages columns\n",
    "    for column in [\"genres\", \"production_countries\", \"spoken_languages\"]:\n",
    "        filtered_spark_df = process_column(filtered_spark_df, column)\n",
    "\n",
    "    filtered_spark_df.show(5)\n",
    "\n",
    "    # Save the DataFrame to DuckDB\n",
    "    ctx.save_to_duckdb(filtered_spark_df, \"letterboxd_movie_ratings_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
