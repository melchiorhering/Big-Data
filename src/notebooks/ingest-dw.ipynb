{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Data Warehouse\n",
    "\n",
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import polars as pl\n",
    "import pyspark\n",
    "import opendatasets as od\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "\n",
    "# from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    OneHotEncoder,\n",
    "    VectorAssembler,\n",
    "    Imputer,\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    ")\n",
    "\n",
    "\n",
    "# from PyMovieDb import IMDB\n",
    "from pyspark.sql import functions as F\n",
    "import duckdb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1\n",
      "0.9.2\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)\n",
    "print(duckdb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duckdb_database = \"../orchestration/db/bigdata.duckdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Cluster Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Connect to Existing Spark Cluster\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# spark = (\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     SparkSession.builder.master(\"spark://spark:7077\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Connect to local Spark Sessions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m     \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYourAppName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\u001b[39;49;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Connect to Existing Spark Cluster\n",
    "# spark = (\n",
    "#     SparkSession.builder.master(\"spark://spark:7077\")\n",
    "#     .appName(\"Spark-ETL\")\n",
    "#     .config(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "#     .getOrCreate()\n",
    "# )\n",
    "\n",
    "# Connect to local Spark Sessions\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"YourAppName\")\n",
    "    # .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to Data Warehouse\n",
    "\n",
    "## Initial Data\n",
    "\n",
    "#### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files that match the pattern\n",
    "csv_files = glob.glob(\"../../data/*.csv\")\n",
    "\n",
    "total_rows = 0\n",
    "imdb_data = None\n",
    "\n",
    "for file_path in csv_files:\n",
    "    print(\"Reading file:\", file_path)\n",
    "    df = (\n",
    "        spark.read.option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(file_path, nullValue=\"\\\\N\")\n",
    "    )\n",
    "    # df.show(5)\n",
    "    total_rows += df.count()\n",
    "\n",
    "    if \"label\" not in df.columns:\n",
    "        df = df.withColumn(\"label\", F.lit(None))\n",
    "\n",
    "    # Rename the column 'primaryTitle' to 'title'\n",
    "    if \"primaryTitle\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"primaryTitle\", \"title\")\n",
    "\n",
    "    if imdb_data is None:\n",
    "        imdb_data = df\n",
    "    else:\n",
    "        imdb_data = imdb_data.union(df)\n",
    "\n",
    "if imdb_data is not None:\n",
    "    imdb_data = imdb_data.drop(\"_c0\")\n",
    "    imdb_data.show(5)\n",
    "    print(f\"Counted Rows {total_rows} vs Final DF {imdb_data.count()}\")\n",
    "\n",
    "# Normalize the strings in the 'title' columns\n",
    "imdb_data = imdb_data.withColumn(\n",
    "    \"title\",\n",
    "    F.lower(F.trim(F.regexp_replace(F.col(\"title\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))),\n",
    ")\n",
    "# Normalize the strings in the 'title' columns\n",
    "imdb_data = imdb_data.withColumn(\n",
    "    \"originalTitle\",\n",
    "    F.lower(F.trim(F.regexp_replace(F.col(\"originalTitle\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation & Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Data\n",
    "validation_hidden = (\n",
    "    (\n",
    "        spark.read.option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(\"../../data/validation_hidden.csv\", nullValue=\"\\\\N\")\n",
    "    )\n",
    "    .drop(\"_c0\")\n",
    "    .select(\n",
    "        \"tconst\",\n",
    "        \"primaryTitle\",\n",
    "    )\n",
    ")\n",
    "# # Select only tconst column\n",
    "# validation_hidden = validation_hidden\n",
    "# Print the dataframe\n",
    "# validation_hidden.show(5)\n",
    "\n",
    "\n",
    "test_hidden = (\n",
    "    (\n",
    "        spark.read.option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(\"../../data/test_hidden.csv\", nullValue=\"\\\\N\")\n",
    "    )\n",
    "    .drop(\"_c0\")\n",
    "    .select(\n",
    "        \"tconst\",\n",
    "        \"primaryTitle\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Select only tconst column\n",
    "# test_hidden = test_hidden.select(\"tconst\")\n",
    "# Print the dataframe\n",
    "# test_hidden.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Polars to retrieve the directing data\n",
    "# Load and parse the JSON file\n",
    "with open(\"../../data/directing.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "movies_polars_df = pl.from_dict(data[\"movie\"]).transpose().rename({\"column_0\": \"movie\"})\n",
    "directors_polars_df = (\n",
    "    pl.from_dict(data[\"director\"]).transpose().rename({\"column_0\": \"director\"})\n",
    ")\n",
    "directing_polars_df = pl.concat(\n",
    "    [\n",
    "        movies_polars_df,\n",
    "        directors_polars_df,\n",
    "    ],\n",
    "    how=\"horizontal\",\n",
    ")\n",
    "# directing_polars_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/writing.json\") as f:\n",
    "    data = json.load(f)\n",
    "writing_json = spark.sparkContext.parallelize(data)\n",
    "writing_spark_df = spark.read.json(writing_json)\n",
    "# writing_spark_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.show(5)\n",
    "imdb_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data\"\n",
    "\n",
    "\n",
    "# # 10000 Data About Movies (1915-2023)\n",
    "# od.download(\n",
    "#     \"https://www.kaggle.com/datasets/willianoliveiragibin/10000-data-about-movies-1915-2023?select=data.csv\",\n",
    "#     data_dir=data_dir,\n",
    "# )\n",
    "\n",
    "# # Movie Industry\n",
    "# od.download(\"https://www.kaggle.com/datasets/danielgrijalvas/movies\", data_dir=data_dir)\n",
    "\n",
    "# # TMDB 10000 Movie Dataset\n",
    "# od.download(\n",
    "#     \"https://www.kaggle.com/datasets/muqarrishzaib/tmdb-10000-movies-dataset\",\n",
    "#     data_dir=data_dir,\n",
    "# )\n",
    "\n",
    "\n",
    "# Full TMDB Movies Dataset 2024\n",
    "# od.download(\n",
    "#     \"https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies\",\n",
    "#     data_dir=data_dir,\n",
    "# )\n",
    "\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/gayu14/tv-and-movie-metadata-with-genres-and-ratings-imbd\",\n",
    "    data_dir=data_dir,\n",
    ")\n",
    "\n",
    "# Golden Globes Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/unanimad/golden-globe-awards/data\",\n",
    "    data_dir=data_dir,\n",
    ")\n",
    "\n",
    "# # Oscar Award Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/unanimad/the-oscar-award\",\n",
    "    data_dir=data_dir,\n",
    ")\n",
    "\n",
    "# FilmTV Movies Dataset\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/stefanoleone992/filmtv-movies-dataset\",\n",
    "    data_dir=data_dir,\n",
    ")\n",
    "\n",
    "# Rotten Tomatoes Top Movies Ratings and Technical Data\n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews\",\n",
    "    data_dir=data_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum value\n",
    "max_value = imdb_data.agg(F.max(\"startYear\")).collect()[0][0]\n",
    "print(\"Max startYear:\", max_value)\n",
    "\n",
    "# Get the minimum value\n",
    "min_value = imdb_data.agg(F.min(\"startYear\")).collect()[0][0]\n",
    "print(\"Min startYear:\", min_value)\n",
    "\n",
    "imdb_data = imdb_data.select(\n",
    "    \"tconst\",\n",
    "    \"title\",\n",
    "    \"originalTitle\",\n",
    "    \"startYear\",\n",
    "    \"runtimeMinutes\",\n",
    "    \"numVotes\",\n",
    "    \"label\",\n",
    ")\n",
    "\n",
    "rename_dict = {}\n",
    "for col_name in imdb_data.columns:\n",
    "    rename_dict[col_name] = \"imdb_\" + col_name\n",
    "\n",
    "imdb_data_rename = imdb_data.withColumnsRenamed(rename_dict)\n",
    "imdb_data_rename.show(5)\n",
    "print(imdb_data_rename.columns)\n",
    "\n",
    "imdb_data_rename.filter(F.col(\"title\") == \"son rise\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMDB Movies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv-and-movie-metadata-with-genres-and-ratings-imbd\n",
    "# tmdb_movies = (\n",
    "#     spark.read.option(\"header\", \"true\")\n",
    "#     .option(\"inferSchema\", \"true\")\n",
    "#     .csv(\"../../data/tmdb-movies-dataset-2023-930k-movies/TMDB_movie_dataset_v11.csv\")\n",
    "#     .drop(\"_c0\")\n",
    "# )\n",
    "# tmdb_movies.show(5)\n",
    "# print(tmdb_movies.columns)\n",
    "\n",
    "# result = imdb_data_rename.join(\n",
    "#     tmdb_movies,\n",
    "#     imdb_data_rename.imdb_title == tmdb_movies.title,\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# result.show(5)\n",
    "# result.count()\n",
    "\n",
    "\n",
    "# result = imdb_data_rename.join(\n",
    "#     tmdb_movies,\n",
    "#     (\n",
    "#         (imdb_data_rename.imdb_title == tmdb_movies.title)\n",
    "#         | (imdb_data_rename.imdb_originalTitle == tmdb_movies.title)\n",
    "#     )\n",
    "#     & (\n",
    "#         (\n",
    "#             F.isnull(imdb_data_rename.imdb_startYear)\n",
    "#             | ~imdb_data_rename(imdb_data_rename.imdb_startYear)\n",
    "#         )\n",
    "#         | (\n",
    "#             (imdb_data_rename.imdb_startYear == tmdb_movies.releaseYearTheaters)\n",
    "#             | (imdb_data_rename.imdb_startYear == tmdb_movies.releaseYearStreaming)\n",
    "#         )\n",
    "#     ),\n",
    "#     \"left\",  # or \"left\", \"right\", \"outer\", depending on what kind of join you want\n",
    "# )\n",
    "\n",
    "\n",
    "# # Count the occurrences of each row\n",
    "# row_counts = result.groupby(result.columns).count()\n",
    "\n",
    "# # Filter the rows that have a count greater than 1\n",
    "# duplicates = row_counts.filter(F.col(\"count\") > 1)\n",
    "\n",
    "# duplicates.show(5)\n",
    "\n",
    "# # Drop duplicates based on the imdb_data_new columns\n",
    "# result = result.dropDuplicates(subset=imdb_data_rename.columns)\n",
    "\n",
    "# result.show(5)\n",
    "# result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata Ratings IMDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tv-and-movie-metadata-with-genres-and-ratings-imbd\n",
    "# movie_metadata_rating = (\n",
    "#     spark.read.option(\"header\", \"true\")\n",
    "#     .option(\"inferSchema\", \"true\")\n",
    "#     .csv(\"../../data/tv-and-movie-metadata-with-genres-and-ratings-imbd/IMBD.csv\")\n",
    "#     .drop(\"_c0\")\n",
    "# )\n",
    "# movie_metadata_rating.show(5)\n",
    "\n",
    "# # Extract the numeric part of the runtime string and convert it to integer\n",
    "# movie_metadata_rating = movie_metadata_rating.withColumn(\n",
    "#     \"runtime\", F.regexp_extract(F.col(\"runtime\"), r\"(\\d+)\", 1).cast(\"integer\")\n",
    "# )\n",
    "# print(f\"Movie Metadata Rating Columsn: {movie_metadata_rating.columns}\")\n",
    "\n",
    "# # Join the dataframes using title and runtime, and also check if a movie name can be joined on imdb_originalTitle if imdb_title is no match\n",
    "# result = imdb_data_rename.join(\n",
    "#     movie_metadata_rating,\n",
    "#     (\n",
    "#         (imdb_data_rename.imdb_title == movie_metadata_rating.movie)\n",
    "#         | (imdb_data_rename.imdb_originalTitle == movie_metadata_rating.movie)\n",
    "#     )\n",
    "#     & (imdb_data_rename.imdb_runtimeMinutes == movie_metadata_rating.runtime),\n",
    "#     how=\"left\",\n",
    "# )\n",
    "# result.show(5)\n",
    "# print(f\"TOTAL ROWS:{result.count()}\")\n",
    "\n",
    "# # # Drop duplicates based on the imdb_data_new columns\n",
    "# result = result.dropDuplicates(subset=imdb_data_rename.columns)\n",
    "# # print(f\"Resulting columns: {result.columns}\")\n",
    "# # result.show(5)\n",
    "# # print(f\"TOTAL ROWS FILTERED DUPLICATES:{result.count()}\")\n",
    "\n",
    "# result = result.select(\n",
    "#     *imdb_data_rename.columns, \"genre\", \"rating\", \"votes\", \"stars\", \"director\"\n",
    "# )\n",
    "# print(f\"Resulting columns: {result.columns}\")\n",
    "# result.show(5)\n",
    "# print(f\"TOTAL ROWS FILTERED DUPLICATES:{result.count()}\")\n",
    "\n",
    "# # Perform a left anti join to get the rows that are not joined\n",
    "# not_joined = imdb_data_rename.join(\n",
    "#     movie_metadata_rating,\n",
    "#     (\n",
    "#         (imdb_data_rename.imdb_title == movie_metadata_rating.movie)\n",
    "#         | (imdb_data_rename.imdb_originalTitle == movie_metadata_rating.movie)\n",
    "#     )\n",
    "#     & (imdb_data_rename.imdb_runtimeMinutes == movie_metadata_rating.runtime),\n",
    "#     how=\"left_anti\",\n",
    "# )\n",
    "# not_joined.show(5)\n",
    "# print(not_joined.count())\n",
    "\n",
    "# # Count the occurrences of each row\n",
    "# row_counts = result.groupby(result.columns).count()\n",
    "\n",
    "# # Filter the rows that have a count greater than 1\n",
    "# duplicates = row_counts.filter(F.col(\"count\") > 1)\n",
    "\n",
    "# duplicates.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FilmtTV Movies Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmt_tv = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../../data/filmtv-movies-dataset/filmtv_movies.csv\")\n",
    "    .drop(\"_c0\")\n",
    ")\n",
    "# Normalize the strings in the 'title' columns\n",
    "filmt_tv = filmt_tv.withColumn(\n",
    "    \"title\", F.lower(F.trim(F.regexp_replace(F.col(\"title\"), \"[^a-zA-Z0-9\\\\s]\", \"\")))\n",
    ")\n",
    "\n",
    "print(filmt_tv.count())\n",
    "filmt_tv.show(5)\n",
    "\n",
    "\n",
    "# Join the dataframes using title and also check if a movie name can be joined on imdb_originalTitle if imdb_title is no match\n",
    "result = imdb_data_rename.join(\n",
    "    filmt_tv,\n",
    "    (\n",
    "        (imdb_data_rename.imdb_title == filmt_tv.title)\n",
    "        | (imdb_data_rename.imdb_originalTitle == filmt_tv.title)\n",
    "    )\n",
    "    & (imdb_data_rename.imdb_startYear == filmt_tv.year),\n",
    "    how=\"left\",\n",
    ")\n",
    "result.show(5)\n",
    "print(result.count())\n",
    "\n",
    "# Drop duplicates based on the imdb_data_new columns\n",
    "result = result.dropDuplicates(subset=imdb_data_rename.columns).drop(\n",
    "    \"notes\", \"description\", \"filmtv_id\", \"year\", \"duration\"\n",
    ")\n",
    "print(f\"Resulting columns: {result.columns}\")\n",
    "result.show(5)\n",
    "print(f\"TOTAL ROWS FILTERED DUPLICATES:{result.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotten Tomatoes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotten_tomato_reviews_data = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\n",
    "        \"../../data/clapper-massive-rotten-tomatoes-movies-and-reviews/rotten_tomatoes_movies.csv\"\n",
    "    )\n",
    "    .drop(\"_c0\")\n",
    ")\n",
    "\n",
    "# Normalize the strings in the 'title' columns\n",
    "rotten_tomato_reviews_data = rotten_tomato_reviews_data.withColumn(\n",
    "    \"title\", F.lower(F.trim(F.regexp_replace(F.col(\"title\"), \"[^a-zA-Z0-9\\\\s]\", \"\")))\n",
    ")\n",
    "\n",
    "rotten_tomato_reviews_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to check if a value is a valid year\n",
    "def is_valid_year(value):\n",
    "    if value is None:\n",
    "        return False\n",
    "    try:\n",
    "        year = int(value)\n",
    "        return 1800 <= year <= 2100\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "is_valid_year_udf = F.udf(is_valid_year, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_rotten_tomato_reviews_data = rotten_tomato_reviews_data.select(\n",
    "    \"title\",\n",
    "    \"writer\",\n",
    "    \"director\",\n",
    "    \"genre\",\n",
    "    \"releaseDateTheaters\",\n",
    "    \"releaseDateStreaming\",\n",
    "    \"audienceScore\",\n",
    "    \"tomatoMeter\",\n",
    "    \"rating\",\n",
    "    \"ratingContents\",\n",
    ")\n",
    "# Extract the year from the release dates\n",
    "selection_rotten_tomato_reviews_data = selection_rotten_tomato_reviews_data.withColumn(\n",
    "    \"releaseYearTheaters\", F.year(F.col(\"releaseDateTheaters\"))\n",
    ")\n",
    "selection_rotten_tomato_reviews_data = selection_rotten_tomato_reviews_data.withColumn(\n",
    "    \"releaseYearStreaming\", F.year(F.col(\"releaseDateStreaming\"))\n",
    ")\n",
    "# Add a prefix to the column names\n",
    "for col_name in selection_rotten_tomato_reviews_data.columns:\n",
    "    selection_rotten_tomato_reviews_data = (\n",
    "        selection_rotten_tomato_reviews_data.withColumnRenamed(\n",
    "            col_name, \"rtm_\" + col_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Handle null values in genre and directors fields\n",
    "result = result.withColumn(\n",
    "    \"genre\", F.when(F.col(\"genre\").isNull(), \"\").otherwise(F.col(\"genre\"))\n",
    ")\n",
    "result = result.withColumn(\n",
    "    \"directors\", F.when(F.col(\"directors\").isNull(), \"\").otherwise(F.col(\"directors\"))\n",
    ")\n",
    "\n",
    "selection_rotten_tomato_reviews_data = selection_rotten_tomato_reviews_data.withColumn(\n",
    "    \"rtm_genre\", F.when(F.col(\"rtm_genre\").isNull(), \"\").otherwise(F.col(\"rtm_genre\"))\n",
    ")\n",
    "selection_rotten_tomato_reviews_data = selection_rotten_tomato_reviews_data.withColumn(\n",
    "    \"rtm_director\",\n",
    "    F.when(F.col(\"rtm_director\").isNull(), \"\").otherwise(F.col(\"rtm_director\")),\n",
    ")\n",
    "\n",
    "# Split the genre and director strings into arrays\n",
    "result = result.withColumn(\"genre\", F.split(F.col(\"genre\"), \",\"))\n",
    "result = result.withColumn(\"directors\", F.split(F.col(\"directors\"), \",\"))\n",
    "\n",
    "selection_rotten_tomato_reviews_data = selection_rotten_tomato_reviews_data.withColumn(\n",
    "    \"rtm_genre\", F.split(F.col(\"rtm_genre\"), \",\")\n",
    ")\n",
    "selection_rotten_tomato_reviews_data = selection_rotten_tomato_reviews_data.withColumn(\n",
    "    \"rtm_director\", F.split(F.col(\"rtm_director\"), \",\")\n",
    ")\n",
    "\n",
    "# Join the dataframes\n",
    "result = result.join(\n",
    "    selection_rotten_tomato_reviews_data,\n",
    "    (\n",
    "        (result.imdb_title == selection_rotten_tomato_reviews_data.rtm_title)\n",
    "        | (result.imdb_originalTitle == selection_rotten_tomato_reviews_data.rtm_title)\n",
    "    )\n",
    "    & (\n",
    "        (F.isnull(result.imdb_startYear) | ~is_valid_year_udf(result.imdb_startYear))\n",
    "        | (\n",
    "            (\n",
    "                result.imdb_startYear\n",
    "                == selection_rotten_tomato_reviews_data.rtm_releaseYearTheaters\n",
    "            )\n",
    "            | (\n",
    "                result.imdb_startYear\n",
    "                == selection_rotten_tomato_reviews_data.rtm_releaseYearStreaming\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    \"left\",  # or \"left\", \"right\", \"outer\", depending on what kind of join you want\n",
    ")\n",
    "\n",
    "# Merge genre and director arrays without creating duplicates\n",
    "result = result.withColumn(\n",
    "    \"genre\",\n",
    "    F.array_join(F.array_distinct(F.concat(F.col(\"genre\"), F.col(\"rtm_genre\"))), \",\"),\n",
    ")\n",
    "\n",
    "result = result.withColumn(\n",
    "    \"directors\",\n",
    "    F.array_join(\n",
    "        F.array_distinct(F.concat(F.col(\"directors\"), F.col(\"rtm_director\"))), \",\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Remove leading commas from genre and directors\n",
    "result = result.withColumn(\"genre\", F.expr(\"substring(genre, 2)\"))\n",
    "result = result.withColumn(\"directors\", F.expr(\"substring(directors, 2)\"))\n",
    "\n",
    "# Drop duplicates based on the imdb_data_new columns\n",
    "result = result.dropDuplicates(subset=imdb_data_rename.columns).drop(\n",
    "    \"rtm_title\",\n",
    "    \"rtm_director\",\n",
    "    \"rtm_genre\",\n",
    "    \"rtm_releaseDateTheaters\",\n",
    "    \"rtm_releaseDateStreaming\",\n",
    "    \"rtm_releaseYearTheaters\",\n",
    "    \"rtm_releaseYearStreaming\",\n",
    ")\n",
    "print(f\"Resulting columns: {result.columns}\")\n",
    "result.show(5)\n",
    "print(f\"TOTAL ROWS FILTERED DUPLICATES:{result.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(\"imdb_tconst\", \"imdb_title\", \"genre\", \"directors\").where(\n",
    "    (F.col(\"genre\").isNotNull()) | (F.col(\"directors\").isNotNull())\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oscars and Golden Globes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oscars\n",
    "oscars = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../../data/the-oscar-award/the_oscar_award.csv\")\n",
    "    .drop(\"_c0\")\n",
    ")\n",
    "# oscars.show(5)\n",
    "# Golden Globes\n",
    "golden_globes = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"../../data/golden-globe-awards/golden_globe_awards.csv\")\n",
    "    .drop(\"_c0\")\n",
    ")\n",
    "# golden_globes.show(5)\n",
    "\n",
    "# Normalize the strings in the 'title' columns\n",
    "oscars = oscars.withColumn(\n",
    "    \"title\", F.lower(F.trim(F.regexp_replace(F.col(\"film\"), \"[^a-zA-Z0-9\\\\s]\", \"\")))\n",
    ")\n",
    "\n",
    "# Normalize the strings in the 'title' columns\n",
    "golden_globes = golden_globes.withColumn(\n",
    "    \"title\", F.lower(F.trim(F.regexp_replace(F.col(\"film\"), \"[^a-zA-Z0-9\\\\s]\", \"\")))\n",
    ")\n",
    "\n",
    "\n",
    "# Count the number of Oscars won\n",
    "oscars_won = (\n",
    "    oscars.filter(F.col(\"winner\") == True)\n",
    "    .groupBy(\"title\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"oscars_won\")\n",
    ")\n",
    "\n",
    "# Count the number of Golden Globes won\n",
    "golden_globes_won = (\n",
    "    golden_globes.filter(F.col(\"win\") == True)\n",
    "    .groupBy(\"title\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"golden_globes_won\")\n",
    ")\n",
    "\n",
    "# Join the result DataFrame with the oscars_won and golden_globes_won DataFrames\n",
    "result = result.join(oscars_won, result.imdb_title == oscars_won.title, \"left\").drop(\n",
    "    \"title\"\n",
    ")\n",
    "result = result.join(\n",
    "    golden_globes_won, result.imdb_title == golden_globes_won.title, \"left\"\n",
    ").drop(\"title\")\n",
    "\n",
    "# Replace null values with 0\n",
    "result = result.fillna(0, subset=[\"oscars_won\", \"golden_globes_won\"])\n",
    "print(\"RESULTING TABLE\")\n",
    "result.show(5)\n",
    "\n",
    "# Filter for movies that won 1 or more Oscars or Golden Globes\n",
    "awards_selection = result.filter(\n",
    "    (F.col(\"oscars_won\") >= 1) | (F.col(\"golden_globes_won\") >= 1)\n",
    ")\n",
    "print(\"FILTER ON WON AWARDS\")\n",
    "awards_selection.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to cast\n",
    "columns_to_cast = [\n",
    "    \"avg_vote\",\n",
    "    \"critics_vote\",\n",
    "    \"public_vote\",\n",
    "    \"total_votes\",\n",
    "    \"humor\",\n",
    "    \"rhythm\",\n",
    "    \"effort\",\n",
    "    \"tension\",\n",
    "    \"erotism\",\n",
    "    \"rtm_audienceScore\",\n",
    "    \"rtm_tomatoMeter\",\n",
    "    \"rtm_rating\",\n",
    "    \"imdb_runtimeMinutes\",\n",
    "    \"imdb_numVotes\",\n",
    "]\n",
    "\n",
    "# Cast each column to float\n",
    "for column in columns_to_cast:\n",
    "    result = result.withColumn(column, F.col(column).cast(\"float\"))\n",
    "\n",
    "# Replace empty strings with NULL in all columns\n",
    "for column in result.columns:\n",
    "    result = result.withColumn(\n",
    "        column, F.when(F.col(column) == \"\", None).otherwise(F.col(column))\n",
    "    )\n",
    "\n",
    "result.show(5)\n",
    "print(result.columns)\n",
    "print(result.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = result.join(\n",
    "    validation_hidden, result.imdb_tconst == validation_hidden.tconst, \"inner\"\n",
    ").drop(*validation_hidden.columns)\n",
    "print(validation_data.count())\n",
    "validation_data.show(5)\n",
    "\n",
    "test_data = result.join(\n",
    "    test_hidden, result.imdb_tconst == test_hidden.tconst, \"inner\"\n",
    ").drop(*test_hidden.columns)\n",
    "test_data.show(5)\n",
    "print(test_data.count())\n",
    "\n",
    "\n",
    "# Create the training data by excluding the validation and test data\n",
    "train_data = result.join(validation_data, [\"imdb_tconst\"], \"left_anti\")\n",
    "train_data = train_data.join(test_data, [\"imdb_tconst\"], \"left_anti\")\n",
    "\n",
    "print(train_data.count())\n",
    "train_data.show(5)\n",
    "\n",
    "print(\"Total Rows:\", train_data.count() + validation_data.count() + test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(\n",
    "    datasets: list[DataFrame], train_data: DataFrame\n",
    ") -> list[DataFrame]:\n",
    "    # Define the stages of the Pipeline\n",
    "    stages = [\n",
    "        StringIndexer(inputCol=\"actors\", outputCol=\"actorsIndex\", handleInvalid=\"keep\"),\n",
    "        OneHotEncoder(inputCol=\"actorsIndex\", outputCol=\"actorsVec\"),\n",
    "        StringIndexer(inputCol=\"country\", outputCol=\"countryIndex\", handleInvalid=\"keep\"),\n",
    "        OneHotEncoder(inputCol=\"countryIndex\", outputCol=\"countryVec\"),\n",
    "        StringIndexer(inputCol=\"genre\", outputCol=\"genreIndex\", handleInvalid=\"keep\"),\n",
    "        OneHotEncoder(inputCol=\"genreIndex\", outputCol=\"genreVec\"),\n",
    "        StringIndexer(\n",
    "            inputCol=\"directors\", outputCol=\"directorsIndex\", handleInvalid=\"keep\"\n",
    "        ),\n",
    "        OneHotEncoder(inputCol=\"directorsIndex\", outputCol=\"directorsVec\"),\n",
    "        StringIndexer(\n",
    "            inputCol=\"rtm_writer\", outputCol=\"writerIndex\", handleInvalid=\"keep\"\n",
    "        ),\n",
    "        OneHotEncoder(inputCol=\"writerIndex\", outputCol=\"writerVec\"),\n",
    "        Imputer(\n",
    "            inputCols=[\n",
    "                \"avg_vote\",\n",
    "                \"critics_vote\",\n",
    "                \"public_vote\",\n",
    "                \"total_votes\",\n",
    "                \"humor\",\n",
    "                \"rhythm\",\n",
    "                \"effort\",\n",
    "                \"tension\",\n",
    "                \"erotism\",\n",
    "                \"rtm_audienceScore\",\n",
    "                \"rtm_tomatoMeter\",\n",
    "                \"imdb_runtimeMinutes\",\n",
    "                \"imdb_numVotes\",\n",
    "            ],\n",
    "            outputCols=[\n",
    "                \"avg_vote\",\n",
    "                \"critics_vote\",\n",
    "                \"public_vote\",\n",
    "                \"total_votes\",\n",
    "                \"humor\",\n",
    "                \"rhythm\",\n",
    "                \"effort\",\n",
    "                \"tension\",\n",
    "                \"erotism\",\n",
    "                \"rtm_audienceScore\",\n",
    "                \"rtm_tomatoMeter\",\n",
    "                \"imdb_runtimeMinutes\",\n",
    "                \"imdb_numVotes\",\n",
    "            ],\n",
    "            strategy=\"median\",\n",
    "        ),\n",
    "        VectorAssembler(\n",
    "            inputCols=[\n",
    "                \"genreVec\",\n",
    "                \"directorsVec\",\n",
    "                \"writerVec\",\n",
    "                \"countryVec\",\n",
    "                \"actorsVec\",\n",
    "                \"avg_vote\",\n",
    "                \"critics_vote\",\n",
    "                \"public_vote\",\n",
    "                \"total_votes\",\n",
    "                \"humor\",\n",
    "                \"rhythm\",\n",
    "                \"effort\",\n",
    "                \"tension\",\n",
    "                \"erotism\",\n",
    "                \"oscars_won\",\n",
    "                \"golden_globes_won\",\n",
    "                \"rtm_audienceScore\",\n",
    "                \"rtm_tomatoMeter\",\n",
    "                \"imdb_runtimeMinutes\",\n",
    "                \"imdb_numVotes\",\n",
    "            ],\n",
    "            outputCol=\"features\",\n",
    "            handleInvalid=\"keep\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Define the Pipeline\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "\n",
    "    # Fit the Pipeline to the training data\n",
    "    model = pipeline.fit(train_data)\n",
    "\n",
    "    # Transform all datasets using the fitted Pipeline\n",
    "    for i in range(len(datasets)):\n",
    "        datasets[i] = model.transform(datasets[i])\n",
    "\n",
    "    # Transform the training data\n",
    "    train_data = model.transform(train_data)\n",
    "\n",
    "    return [train_data] + datasets\n",
    "\n",
    "\n",
    "# Apply the preprocessing function to each dataset\n",
    "train_data, validation_data, test_data = preprocess_datasets(\n",
    "    [validation_data, test_data], train_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST CLASSFIER\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Convert boolean to integer\n",
    "train_data = train_data.withColumn(\"imdb_label\", F.col(\"imdb_label\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = SparkXGBClassifier(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"imdb_label\",\n",
    "    num_workers=5,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(model.max_depth, [3, 4])\n",
    "    .addGrid(model.learning_rate, [0.01, 0.1, 0.2])\n",
    "    .addGrid(model.n_estimators, [100])\n",
    "    .addGrid(model.colsample_bytree, [0.3, 0.7])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"imdb_label\")\n",
    "\n",
    "# Define the cross-validation\n",
    "cv = CrossValidator(\n",
    "    estimator=model,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=2,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "cv_model = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_validation = model.transform(validation_data).select(\n",
    "    \"imdb_tconst\",\n",
    "    \"imdb_title\",\n",
    "    \"rawPrediction\",\n",
    "    \"probability\",\n",
    "    F.when(F.col(\"prediction\") == 1, \"True\").otherwise(\"False\").alias(\"prediction\"),\n",
    ")\n",
    "\n",
    "predict_validation.show(5)\n",
    "predict_validation.select(\"prediction\").coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    \"../../output/validate_predictions.csv\",\n",
    "    header=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkXGBClassifier' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m(test_data)\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_tconst\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_title\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     F\u001b[38;5;241m.\u001b[39mwhen(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m predict_test\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     10\u001b[0m predict_test\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../output/test_predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkXGBClassifier' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "predict_test = model.transform(test_data).select(\n",
    "    \"imdb_tconst\",\n",
    "    \"imdb_title\",\n",
    "    \"rawPrediction\",\n",
    "    \"probability\",\n",
    "    F.when(F.col(\"prediction\") == 1, \"True\").otherwise(\"False\").alias(\"prediction\"),\n",
    ")\n",
    "predict_test.show(5)\n",
    "\n",
    "predict_test.select(\"prediction\").coalesce(1).write.mode(\"overwrite\").csv(\n",
    "    \"../../output/test_predictions.csv\", header=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
